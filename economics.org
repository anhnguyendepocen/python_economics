#+Title: Doing economics with python
#+Author: Jan Boone
#+LANGUAGE:  en
#+INFOJS_OPT: view:showall toc:t ltoc:t mouse:underline path:http://orgmode.org/org-info.js
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../css/notebook.css" />
#+LaTeX_CLASS: article-nodefaults
#+LaTeX_HEADER: \usepackage{sectsty}
#+LaTeX_HEADER: \sectionfont{\normalfont\scshape}
#+LaTeX_HEADER: \subsectionfont{\normalfont\itshape}
#+latex_header: \usepackage[round,authoryear]{natbib}
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+OPTIONS: \n:nil @:t ::t |:t ^:{} _:{} *:t TeX:t LaTeX:t
#+STARTUP: showall
#+LaTeX: \maketitle
#+OPTIONS: d:(not "ANSWER")

* Introduction

This is the notebook to accompany the course Applied Economic Analysis at Tilburg University. The idea is to bring economic concepts "alive" by programming them in python. The choice of topics is loosely based on cite:tirole_2017.

The point is not that we go into models in detail. Instead, we sketch the trade offs and then model these in python. We make graphs to explain the intuition. In this way, you learn both python and to use economics in a more "broad brush" fashion than "taking the third derivative".

When we go through the notes below, there will be *Questions* and *Assignments*. Questions we will typically do in class together. Assignments will take more time and you do these at home. If you have questions about these, come back to it in class.


* Python packages that we will be using

We need the following packages to run the code below. If you get an error saying that the package is not available, you can install it using either ~pip install~ or ~conda install~.

#+BEGIN_SRC ipython :exports code
import pandas as pd
import numpy as np
import pymc3 as pm
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats, optimize
import random
import wbdata as wb

plt.style.use('seaborn')
%matplotlib inline
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[2]:
# output
: /Users/boone/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
:   from ._conv import register_converters as _register_converters
: 
:END:

* The market

This part is based on Tirole chapters: 1, 2, 6.

** why do we love the market?

Many (but not all) economists love the market as an organizing institution. This affection for markets is traced back by some to Adam Smith and his [[https://en.wikipedia.org/wiki/Invisible_hand]["invisible hand"]]. Others may not think that the market is so great, but basically distrust other organizing institutions like a government. They will point, for instance, to the fall of communism in the [[https://en.wikipedia.org/wiki/Revolutions_of_1989][former Soviet Union]].

Although you, as an economist, have seen many market models already,
it may be illustrative to go back to basics. To understand the
advantages of the market, let us consider a very simple economy. We
focus on one type of product and there is an exogenous endowment of
this product (supply) equal to ~number_of_goods~. Further, in this
economy there are ~number_of_agents~ agents who have a valuation for
this good which is randomly drawn from a normal distribution. An agent
can at max. consume one unit of the product and her utility is then
given by her valuation. The value of consuming zero units and the
additional value of consuming more than one unit both equal 0.

The vector ~valuations~ contains for each agent her valuation and we sort this vector such that the first agent has the highest valuation.

#+BEGIN_SRC ipython
number_of_agents = 1000
number_of_goods = 100

valuations = sorted(pm.Normal.dist(100,20).random(size=number_of_agents),reverse = True)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[55]:
:END:

Note that we are using [[http://docs.pymc.io/notebooks/getting_started][pymc3]] here to generate random numbers from a distribution. There are also other python libraries which can do this, e.g [[https://scipy.org/][scipy]]. We use pymc3 as we will use it later for estimation as well.


**Question** What is the economic name for the following expression? To answer this question, you need to understand both how indexing works in python and which economic concept is captured by this expression.

#+BEGIN_SRC ipython
print("{0:.2f}".format(valuations[number_of_goods]))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[56]:
# output
: 125.21
: 
:END:

Suppose that we have an omniscient social planner who knows the valuation of every agent in the economy. This planner aims to maximize the (unweighted) sum of agents' utilities.

**Question** Calculate the total welfare that this planner can achieve. Denote this value ~max_welfare~.

#+BEGIN_SRC ipython :exports none
max_welfare = np.sum(valuations[:number_of_goods])
print("{0:.2f}".format(max_welfare))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[57]:
# output
: 13464.87
: 
:END:

Hence, this is the best that we can do. It gives us an upperbound on the welfare that can be achieved. Having an omniscient social planner seems unrealistic, but perhaps there is an institution that can achieve this outcome without omniscient intervention. You guessed it...


*** market outcome

Now we compare the maximum welfare that a planner can achieve with the market outcome.

**Question** Define a function ~demand(p,valuations)~ which has as arguments a price $p$ and a vector of agents' valuations. This function returns the number of agents who are willing to buy the product at price $p$. Since each agent who buys, buys exactly one unit in our set up, this function returns demand at each price.

#+BEGIN_SRC ipython :exports none
def demand(p,valuations):
    return sum(valuations>p)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[58]:
:END:

**Question** Using ~matplotlib~ plot (fixed) supply and the function ~demand~ against price, where we maintain the economic convention of having quantity on the horizontal axis and price on the vertical axis.

#+BEGIN_SRC ipython :exports none
range_p = np.arange(60,150)

plt.plot([demand(p,valuations) for p in range_p],range_p, label = "demand")
plt.plot([number_of_goods for p in range_p],range_p, label="supply")
plt.legend()
plt.xlabel("$Q$")
plt.ylabel("$P$")
plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[59]:
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-d0XxXL.png]]
:END:

In order to calculate the equilibrium price, we define a function ~excess_demand~. We will then look for the price where ~excess_demand~ equals 0; this is the equilibrium price.

#+BEGIN_SRC ipython
def excess_demand(p,valuations,number_of_goods):
    return demand(p,valuations)-number_of_goods
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[60]:
:END:

In order to find the equilibrium price, we use from ~scipy.optimize~ the function [[https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fsolve.html][fsolve]]. If you want to know more about solve, there are people at [[https://stackoverflow.com/questions/8739227/how-to-solve-a-pair-of-nonlinear-equations-using-python][stackoverflow]] discussing this function. Stackoverflow is generally a great resource if you are wondering how to solve a problem in python.

#+BEGIN_SRC ipython
price = optimize.fsolve(lambda x: excess_demand(x,valuations,number_of_goods),120)
print("{0:.2f}".format(price))
#+END_SRC


So, now we know the equilibrium price

**Question** Calculate total welfare at this equilibrium price.


#+BEGIN_SRC ipython :exports none
np.sum(valuations[:demand(price,valuations)])
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[62]:
# text/plain
: 13464.866134617674
:END:


**Question** How does this welfare compare to the maximum welfare that the omniscient social planner can achieve? Recall that this level is:

#+BEGIN_SRC ipython
"{0:.2f}".format(max_welfare)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[93]:
# text/plain
: '13464.87'
:END:

*** elastic demand and supply

Up till now we assumed that supply was inelastic: there was a given endowment of goods and this was auctioned off to consumers. Now we assume that some agents initially own the goods. However, these agents are not necessarily the ones that value the goods the most.

In particular, we give ~number_of_goods~ agents one unit of the good. They become suppliers.

#+BEGIN_SRC ipython
random.shuffle(valuations)
valuations_supply = valuations[:number_of_goods]
valuations_demand = valuations[number_of_goods:]
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[64]:
:END:


**Question** Define a function ~supply~ which depends on the price and the valuations of the suppliers.

#+BEGIN_SRC ipython :exports none
def supply(p,valuations):
    return sum(valuations<p)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[65]:
:END:

**Question** Use ~matplotlib~ to plot demand and supply in a single figure. 

#+BEGIN_SRC ipython :exports none
range_p = np.arange(60,150)

plt.plot([demand(p,valuations_demand) for p in range_p],range_p, label = "demand")
plt.plot([supply(p,valuations_supply) for p in range_p],range_p, label="supply")
plt.legend()
plt.xlabel("$Q$")
plt.ylabel("$P$")
plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[66]:
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-znV4gM.png]]
:END:


**Question** Define the function ~demand_minus_supply~ which looks
like ~excess_demand~ above but now with elastic supply. The function
depends on the price, the valuations of people demanding the good and
the valuations of people supplying it.

Then use ~fsolve~ to find the equilibrium price.

#+BEGIN_SRC ipython :exports none
def demand_minus_supply(p,valuations_demand,valuations_supply):
    return demand(p,valuations_demand)-supply(p,valuations_supply)

optimize.fsolve(lambda x: demand_minus_supply(x,valuations_demand,valuations_supply),120)



#+END_SRC

#+RESULTS:
:RESULTS:
# Out[67]:
# text/plain
: array([125.27357427])
:END:

**Question** How does the equilibrium price here compare to the equilibrium price above with exogenous supply? Is the price here higher? Why (not)? Is welfare higher here than above?

#+BEGIN_SRC ipython :exports none
price
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[68]:
# text/plain
: array([125.27357427])
:END:

:ANSWER: 
It is the same: either suppliers will consume themselves (then they would have done so above as well) or they sell; if they sell, they sell to the same agents as above (with highest valuation).
:END:


** why do others not love the market?

Although the results above look great, the assumptions we made, may not be realistic in every market. To illustrate, without saying so, we assumed above that the market is perfectly competitive and without external effects. Here we program three reasons why the market outcome may not necessarily lead to maximum welfare. First, we look at income inequality and the problem that this causes for the market. Then we consider market power and finally we model external effects.

*** income distribution

In micro economics we usually do not do much with income distributions. Often because models where income distributions play a role are tricky to solve analytically. But here we program/simulate and hence we do not worry about analytical solutions.

Now in addition to the valuations introduced above (the utility an agent gets from consuming the good), we need an income distribution. The former determines the willingness to pay (wtp) for an agent, the latter the price an agent can pay. A consumer is willing to buy the product at a price $p$ if both her wtp and her income exceed $p$.

First, we randomly draw an income for each agent in the economy.

#+BEGIN_SRC ipython
incomes = pm.Normal.dist(100,20).random(size=number_of_agents)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[69]:
:END:

Next, we need to redefine demand, now denoted ~demand_2~ which takes into account both whether an agent values the good more than $p$ and whether she can afford $p$.

#+BEGIN_SRC ipython
def afford(p,incomes):
    return incomes>p

def wtp(p,valuations):
    return valuations>p

def demand_2(p,valuations,incomes):
    return np.sum(afford(p,incomes)*wtp(p,valuations))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[70]:
:END:


**Question** Define the function ~excess_demand_2~ which depends on $p$, agents' valuations, incomes and number of goods (which we assume to be inelastically supplied again).

#+BEGIN_SRC ipython :exports none
def excess_demand_2(p,valuations,incomes,number_of_goods):
    return demand_2(p,valuations,incomes)-number_of_goods
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[71]:
:END:

**Question** Use ~fsolve~ to determine the equilibrium price in this case. Is this price higher or lower than above? Why?

:ANSWER:
  price is always lower because income constraint binds; agents always pay less, never more
:END:

#+BEGIN_SRC ipython :exports none
price_2 = optimize.fsolve(lambda x: excess_demand_2(x,valuations,incomes,number_of_goods),120)
print(price_2)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[72]:
# output
: [110.17332081]
: 
:END:

#+BEGIN_SRC ipython :exports none
price
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[73]:
# text/plain
: array([125.27357427])
:END:

**Question** Calculate welfare in the market equilibrium. How does it compare to ~max_welfare~?

#+BEGIN_SRC ipython :exports none
welfare_2 = np.sum(afford(price_2,incomes)*wtp(price_2,valuations)*valuations)
print(welfare_2)
print(max_welfare)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[74]:
# output
: 12228.096806592494
: 13464.866134617674
: 
:END:


**Assignment** Model an economy where an increase in income inequality reduces welfare.

:ANSWER:
With 100 goods and 1000 agents, only few agents buy the good. By increasing the variance, some high value agents may actually get a higher income due to increased inequality. This can increase welfare. If we have 100 goods and 150 agents, the last agent to buy has income below the mean (100). Increasing inequality will tend to reduce this agent's income. This reduces the equilibrium price and hence welfare.
:END:

#+BEGIN_SRC ipython :exports none
number_of_agents_2 = 150
valuations_2 = sorted(pm.Normal.dist(100,20).random(size=number_of_agents_2),reverse = True)

income_std = 20
incomes_20 = pm.Normal.dist(100,income_std).random(size=number_of_agents_2)
price_20 = optimize.fsolve(lambda x: excess_demand_2(x,valuations_2,incomes_20,number_of_goods),80)
print(np.sum(afford(price_20,incomes_20)*wtp(price_20,valuations_2)*valuations_2))

income_std = 40
incomes_40 = pm.Normal.dist(100,income_std).random(size=number_of_agents_2)
price_40 = optimize.fsolve(lambda x: excess_demand_2(x,valuations_2,incomes_40,number_of_goods),80)
print(np.sum(afford(price_40,incomes_40)*wtp(price_40,valuations_2)*valuations_2))


#+END_SRC

#+RESULTS:
:RESULTS:
# Out[75]:
# output
: 10667.846675240511
: 10314.577161570669
: /Users/boone/anaconda3/lib/python3.6/site-packages/scipy/optimize/minpack.py:163: RuntimeWarning: The iteration is not making good progress, as measured by the 
:   improvement from the last ten iterations.
:   warnings.warn(msg, RuntimeWarning)
: 
:END:



*** market power

**Warning** We are going to do a couple of things wrong in this section. No need to panic; this actually happens a lot when you are programming. Use your economic intuition to see where the mistakes are and correct them.

Suppose that we now give all the products to 1 agent who then owns ~number_of_goods~ units of this good. To simplify, we assume that this agent values the good at 0.

**Question** Suppose we use the function ~demand_minus_supply~ defined above to calculate the equilibrium price. Would the equilibrium price increase due to market power? Why (not)?


Perhaps a monopolist would not use an auction to sell all the goods. Let's calculate the profits of the monopolist as a function of the price and the valuations of the agents.

#+BEGIN_SRC ipython
def profit(p,valuations):
    return p*demand(p,valuations)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[76]:
:END:


#+BEGIN_SRC ipython
range_p = np.arange(0,140)

plt.plot(range_p, [profit(p,valuations) for p in range_p], label = "profit")
plt.legend()
plt.xlabel("$P$")
plt.ylabel("$\pi$")
plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[77]:
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-cTecbZ.png]]
:END:

It looks like the profit maximizing price is around 80. Recall the equilibrium price under perfect competition above:


#+BEGIN_SRC ipython
price
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[78]:
# text/plain
: array([125.27357427])
:END:


**Question** Since when does a monopolist charge a lower price than a perfectly competitive market?



**Assignment** Calculate the profit maximizing price in this case.


#+BEGIN_SRC ipython :exports none
def profit(p,valuations):
    return p*min(demand(p,valuations),number_of_goods)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[79]:
:END:


#+BEGIN_SRC ipython :exports none
range_p = np.arange(120,140)

plt.plot(range_p, [profit(p,valuations) for p in range_p], label = "profit")
plt.legend()
plt.xlabel("$P$")
plt.ylabel("$\pi$")
plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[80]:
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-8EdGjp.png]]
:END:


:ANSWER:
Is it possible that monop. price equals perf. compet. price? yes it is, if loss at the margin (from increasing $p$) exceeds the gain of the price increase. In the model we can change this by introducing a production cost equal to, say, 120. This cost does not affect the perfect compet. outcome (as $p>120$) but by reducing the margin, the monopolist willing to sell less in order to charge a higher price. In the function profit, we get $(p-120)$ instead of $p$ times quantity.
:END:



*** merger simulation

This part is based on Tirole chapter 13.

In this section, we model a more standard oligopoly market with
Cournot competition. We start with three firms and then calculate what
happens if two firms merge such that only two firms are left in the
industry. Hence, we first calculate the equilibrium with three firms,
denoted by 1, 2 and 3. Then firms 2 and 3 merge so that we are left with 2 firms; denoted by
1 and 2.

We are interested in the effects of the merger on the equilibrium price.

We assume that before the merger each firm has constant marginal costs
equal to 0.3. We assume a simple linear (inverse) demand curve of the
form $p=1-Q$ where $p$ denotes price and $Q$ total output on the market.
Total output equals the sum of each firm's output: $Q= q_1 + q_2+q_3$.

The function ~reaction~ gives the optimal reaction of a firm to the total output ~Q_other~ from its competitors. In this function, we use the routine [[https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fminbound.html][fminbound]]. Python does not have maximization routines, hence we minimize "minus profits" (which is the same from a mathematical point of view). The parameters ~0,1~ in this routine give the bounds over which we optimize. Since demand is of the form $p(Q)=1-Q$, we know that no firm will choose $q>1$; further we also know that $q \geq 0$.

The fixed point makes sure that for each of the three firms, their output level is equal to its optimal reaction to the output levels of its competitors. If each firm plays its optimal response, given the actions of the other players, we have a Nash equilibrium.

#+BEGIN_SRC ipython
c0 = 0.3
vector_c = [c0]*3

def p(Q):
    return 1 - Q

def costs(q,c):
    return c*q

def profits(q,Q_other,c):
    return p(q+Q_other)*q-costs(q,c)

def reaction(Q_other,c):
    q1 =  optimize.fminbound(lambda x: -profits(x,Q_other,c),0,1,full_output=1)
    return q1[0]

def fixed_point_three_firms(vector_q,vector_c):
    return [vector_q[0]-reaction(vector_q[1]+vector_q[2],vector_c[0]),
            vector_q[1]-reaction(vector_q[0]+vector_q[2],vector_c[1]),
            vector_q[2]-reaction(vector_q[0]+vector_q[1],vector_c[2])]

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[81]:
:END:

We calculate the equilibrium output level, price and the Herfindahl index. The Herhindahl index is defined as the sum of squared market shares:

\begin{equation}
\label{eq:1}
H = \sum_j \left( \frac{q_j}{\sum_i q_i} \right)^{2}
\end{equation}

If we have $n$ symmtric firms, we have $H = 1/n$. Hence, more competition in the form of more firms in the market leads to a lower Herfindahl index.

#+BEGIN_SRC ipython
initial_guess_3 = [0,0,0]

Q0 = np.sum(optimize.fsolve(lambda q: fixed_point_three_firms(q,vector_c), initial_guess_3))
P0 = p(Q0)
H0 = 3*(1.0/3.0)**2

print("Before the merger")
print("=================")
print("total output: {:.3f}".format(Q0))
print("equil. price: {:.3f}".format(P0))
print("Herfn. index: {:.3f}".format(H0))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[82]:
# output
: Before the merger
: =================
: total output: 0.525
: equil. price: 0.475
: Herfn. index: 0.333
: 
:END:


**Question** Define a function ~fixed_point_two_firms~ with the same
structure as the function ~fixed_point_three_firms~ above, except that
it derives the equilibrium output levels for a duopoly (two firms).
Test this function by showing that each of the two firms produces
0.3333 in case both firms have zero costs; use ~fsolve~ as above.

#+BEGIN_SRC ipython :exports none
def fixed_point_two_firms(vector_q,vector_c):
    return [vector_q[0]-reaction(vector_q[1],vector_c[0]),
            vector_q[1]-reaction(vector_q[0],vector_c[1])]

initial_guess = [0,0]

optimize.fsolve(lambda q: fixed_point_two_firms(q,[0,0]), initial_guess)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[83]:
# text/plain
: array([0.33333333, 0.33333333])
:END:

A competition authority (CA) is asked to evaluate the effects
of a merger between firms 2 and 3. Firms 2 and 3 claim that by merging
they can reduce their constant marginal costs. But it is not clear by
how much they will reduce their costs.

The CA assumes that the marginal cost level of the merged firm is
uniformly distributed between 0 and the current marginal cost level
~c0~. The merger will not affect the marginal cost level of firm 1 which
does not merge. Firm 1's cost level remains ~c0~.

The next cell generates a vector of cost levels for the merged firm,
denoted ~c_after_merger~. Then it calculates the equilibrium output
levels for (the non-merging) firm 1 and (the merged) firm 2.

#+BEGIN_SRC ipython
c_after_merger = pm.Uniform.dist(0,c0).random(size = 100)

initial_guess = [0.2,0.2]

q1_after_merger = [optimize.fsolve(lambda q: fixed_point_two_firms(q,[c0,c]), initial_guess)[0] for c in c_after_merger]
q2_after_merger = [optimize.fsolve(lambda q: fixed_point_two_firms(q,[c0,c]), initial_guess)[1] for c in c_after_merger]
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[84]:
:END:

**Question** Create a dataframe called ~df_after_merger~ with
three columns: ~c_merged_firm~, ~output_non_merging_firm~,
~output_merged_firm~ containing resp. the cost level of the merged firm,
the output level of firm 1 and the output level of firm 2.

#+BEGIN_SRC ipython :exports none
df_after_merger = pd.DataFrame({'c_merged_firm': c_after_merger, 
                                'output_non_merging_firm': q1_after_merger,
                                'output_merged_firm': q2_after_merger})
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[85]:
:END:

**Question** Add three columns to the dataframe with resp. total
equilibrium output on the market, ~Q~, equilibrium price, ~P~ and the
Herfindahl index, ~H~.

#+BEGIN_SRC ipython :exports none
df_after_merger['Q'] = df_after_merger.output_non_merging_firm + df_after_merger.output_merged_firm
df_after_merger['P'] = p(df_after_merger.Q)
df_after_merger['H'] = (df_after_merger.output_non_merging_firm/df_after_merger.Q)**2+(df_after_merger.output_merged_firm/df_after_merger.Q)**2
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[86]:
:END:

**Question** Make a histogram of the equilibrium price ~P~ after
the merger. Also indicate in the histogram the equilibrium price before
the merger ~P0~. Label the horizontal axis with $P$.

[hint: you may want to use matplotlib's ~hist~, ~vlines~ and ~legend~ to
make this graph (e.g use google to find these functions); but feel free
to use something else]

#+BEGIN_SRC ipython :exports none
plt.hist(df_after_merger.P, bins = 30, density = 1, label = 'after merger')
plt.vlines(P0,0,25, color = 'red', label = 'before merger')
plt.legend()
plt.xlabel('$P$')
plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[87]:
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-2Nu1P7.png]]
:END:


**Excersise** Explain why sometimes the equilibrium price after
the merger exceeds the equilibrium price before the merger and sometimes
it is lower than the pre-merger price.

What is calculated in the following cell?

#+BEGIN_SRC ipython
np.sum(df_after_merger.P < P0)/len(df_after_merger.P)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[88]:
# text/plain
: 0.32
:END:


**Question** Make a graph with the Herfindahl index on the
horizontal axis and the equilibrium price on the vertical axis. This is
straightforward for $(H,P)$ after the merger as both values are in the
dataframe. Add in another color, the pre-merger combination ~(H0,P0)~
that we calculated above.

#+BEGIN_SRC ipython :exports none
plt.scatter(df_after_merger.H,df_after_merger.P,label='after merger')
plt.scatter(H0,P0,label='pre merger')
plt.legend()
plt.xlabel('$H$')
plt.ylabel('$P$')
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[89]:
# text/plain
: Text(0,0.5,'$P$')

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-fcWaFf.png]]
:END:


**Question** What does the figure above illustrate about the relation
between the Herfindahl index and the equilibrium price? To illustrate,
some people think that lower values of the Herfindahl index are
associated with more competitive outcome. Would you agree with this?

*** external effects

A final reason why people are not always enthusiastic about markets is the presence of external effects. One can think of pollution associated with the production of a good. We model this as follows. Assume a monopolist can produce the product at cost $c q$. But production leads to an external effect equal to $\gamma q$. Hence, the social cost of production equals $(c+\gamma)q$

We can model this as follows. 

#+BEGIN_SRC ipython
number_of_agents = 1000
valuations = np.array(sorted(pm.Normal.dist(100,20).random(size=number_of_agents),reverse = True))

def demand(p,valuations):
    return sum(valuations>p)

c = 30
γ = 80
def costs(q):
    return c*q

def externality(q):
    return γ*q

def profit_c(p,valuations):
    return p*demand(p,valuations)-costs(demand(p,valuations))

def welfare_e(p,valuations):
    return np.sum(valuations[:demand(p,valuations)])-costs(demand(p,valuations))-externality(demand(p,valuations))


#+END_SRC

#+RESULTS:
:RESULTS:
# Out[90]:
:END:

**Question** Show graphically that the welfare maximizing price exceeds the profit maximizing price.


#+BEGIN_SRC ipython :exports none
range_p = np.arange(60,150)

plt.plot(range_p, [profit_c(p,valuations) for p in range_p], label = "profit")
plt.plot(range_p, [welfare_e(p,valuations) for p in range_p], label = "welfare")
plt.legend()
plt.xlabel("$P$")
plt.ylabel("$\pi$, welfare")
plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[91]:
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-mBAZru.png]]
:END:


**Question** What is the interpretation of this result? Which policy instrument can the government use here?


* Asymmetric information

Tirole chapter 15.

One of the reasons why markets (or other institutions for that matter) work less well than a naive observer may think is asymmetric information. We consider here both adverse selection and moral hazard. Adverse selection we analyze in the context of insurance and moral hazard in the context of taxation.


** adverse selection

**Question** What is adverse selection?

Consider an economy with ~number_of_agents~ agents. Each agent has an endowment/income equal to ~income~ and faces a potential loss of the size ~cost~. Agents differ in the probability $\pi$ of this loss. We randomly draw 50 values for $\pi$ assuming it is uniformly distributed on $[0,1]$.

Further, agents have a utility function of the form $u(x)=x^{\rho}$.

#+BEGIN_SRC ipython
income = 1.1
cost = 1
ρ = 0.1
def u(x):
    return x**ρ

number_of_agents = 50

π = pm.Uniform.dist(0.0,1.0).random(size = number_of_agents)
π.sort()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[94]:
:END:

Since we assume that $\rho \in \langle 0, 1 \rangle$, agents are risk averse and would like to buy insurance which covers the loss. We assume that insurance covers the loss completely at a premium $\sigma$. As we assume that the probability of loss, $\pi$, is exogenous, there is no reason to have co-payments of any sort.

An agent buys insurance if and only if

\begin{equation}
\label{eq:2}
u(\text{income}-\sigma) > \pi u(\text{income}-\text{cost}) + (1-\pi) u(\text{income})
\end{equation}

**Question** Define a function ~insurance_demand~ that returns the number of agents buying insurance as a function of the premium $\sigma$.

#+BEGIN_SRC ipython :exports none
def insurance_demand(σ):
    return np.sum(u(income-σ)-(π*u(income-cost)+(1-π)*u(income))>0)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[95]:
:END:

We assume that this insurance market is perfectly competitive. That is, for each quantity supplied, insurance companies compete down the price such that the premium equals the average (expected) cost of the agents buying insurance.

**Question** Explain the code of the following function.

#+BEGIN_SRC ipython
def insurance_supply(Q):
    return np.mean(π[-Q:])*cost
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[96]:
:END:

We plot demand and supply in one figure. In addition, we plot the marginal costs curve.

#+BEGIN_SRC ipython
range_Q = np.arange(1,number_of_agents+1,1)
range_sigma = np.arange(0,1.01,0.01)
plt.plot(range_Q,[insurance_supply(Q) for Q in range_Q],label="insurance supply")
plt.plot([insurance_demand(sigma) for sigma in range_sigma],range_sigma,label="insurance demand")
plt.plot(range_Q,[π[-Q]*cost for Q in range_Q],label="marginal cost")
plt.legend()
plt.xlabel('$Q$')
plt.ylabel('$\sigma$')
plt.title('Perfectly competitive insurance market')
plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[97]:
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-LeKCIU.png]]
:END:

**Question** Interpret this figure. In particular, 
+ explain why all curves are downward sloping (is supply not usually upward sloping?)
+ what is approx. the equilibrium premium $\sigma$?
+ is the market outcome efficient?
+ what can we learn from the marginal cost curve?


**Assignment** Show graphically the effect of an increase in income on the market outcome. Does the inefficiency increase or decrease with income? Why?

#+BEGIN_SRC ipython :exports none
income = 2

def insurance_demand(σ):
    return np.sum(u(income-σ)-(π*u(income-cost)+(1-π)*u(income))>0)
plt.plot(range_Q,[insurance_supply(Q) for Q in range_Q],label="insurance supply")
plt.plot([insurance_demand(sigma) for sigma in range_sigma],range_sigma,label="insurance demand")
plt.plot(range_Q,[π[-Q]*cost for Q in range_Q],label="marginal cost")
plt.legend()
plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[98]:
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-cxp0iX.png]]
:END:


** moral hazard: optimal taxation 

With moral hazard, agents take hidden actions. The actions that they take are affected by the incentives that they face. We consider this in the context of taxation. 

People differ in their productivity. For some people it is easy to generate a gross income $x$, for others generating such an income would be very costly in terms of effort. In the real world, such differences in productivity can be caused by IQ, education, health status etc. Here, we simply model this as an effort cost. People with a high effort cost have lower productivity than people with low effort costs. We assume that the effort cost is log-normally distributed. 

The government uses a linear tax schedule: $\tau x - \tau_0$. Hence, when you have a gross income $x$, your net income equals $(1-\tau)x+\tau_0$. Where we assume that for the economy as a whole the tax revenue is redistributed among the population. Hence, ~number_of_agents~ times $\tau_0$ has to equal the total revenue from the marginal tax rate $\tau$.

Agents maximize their utility by choosing production $x$:

\begin{equation}
\label{eq:3}
\max_{x \geq 0} (1-\tau)x+\tau_0 - cx^2
\end{equation}

where agents differ in $c$ and $c$ is not observable.
 
These two aspects are important: if $c$ were observable or if everyone was symmetric (had the same $c$) taxation would be easy. To see why, first note that income $x$ is apparently observable since taxation depends on it. Hence, the government could say to an agent $c$: I want you to produce income $x$ and you give me a share $\tau$ of this income. 

In our set-up with heterogeneity in $c$ and $c$ unobservable, the government cannot force people to generate income $x$ because some of these agents may have such a high $c$ that this is inefficient (or even impossible).

Hence, the government sets the tax schedule (in our case linear) and allows each agent to choose her own production level. The higher $\tau$, the lower an agent's production will be.

#+BEGIN_SRC ipython
number_of_agents = 200
effort_costs = pm.Lognormal.dist(mu=0.0,sd=0.5).random(size=number_of_agents)
def effort(c,τ):
    sol = optimize.minimize(lambda x: -(x*(1-τ)-c*x**2),1)
    return sol.x
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[99]:
:END:

We use the following welfare function:

\begin{equation}
\label{eq:4}
W = \left(\sum_i ( (1-\tau)x_i + \tau_0 - c_i x_i^2)^{\rho} \right)^{1/\rho}
\end{equation}

With $\rho=1$, the social planner just maximizes the sum of utility. With $\rho<1$, the planner has a taste for redistribution: agents with low utility get a relatively high weight in this welfare function.

The function ~Welfare~ first calculates for a given $\tau$, what the value of $\tau_0$ is (using budget balance for the government). Then for these values of $\tau$ and $\tau_{0}$, $W$ is calculated.

#+BEGIN_SRC ipython
def Welfare(τ,ρ):
    τ_0 = np.mean([τ*effort(c,τ) for c in effort_costs])
    return (np.sum([((1-τ)*effort(c,τ)+τ_0 - c*effort(c,τ)**2)**ρ for c in effort_costs]))**(1/ρ)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[100]:
:END:

**Question** Plot ~Welfare~ as a function of $\tau$ for $\rho=1$. What is the welfare maximizing tax rate? Why?

#+BEGIN_SRC ipython :exports none
range_tax = np.arange(0,1.1,0.1)
plt.plot(range_tax,[Welfare(τ,1) for τ in range_tax])
plt.xlabel('$\\tau$')
plt.ylabel('$W$')
plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[101]:
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-yDPPAz.png]]
:END:

**Question** What happens to the optimal tax rate as $\rho<1$ falls?

#+BEGIN_SRC ipython :exports none
range_tax = np.arange(0,1.1,0.1)
plt.plot(range_tax,[Welfare(τ,-1.5) for τ in range_tax], label="$\\rho=-1.5$")
plt.plot(range_tax,[Welfare(τ,-1.9) for τ in range_tax], label="$\\rho=-1.9$")
plt.xlabel('$\\tau$')
plt.ylabel('$W$')
plt.legend()
plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[102]:
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-oycrTw.png]]
:END:



**Assignment** Redefine the function ~Welfare~ above such that it uses [[https://en.wikipedia.org/wiki/A_Theory_of_Justice][Rawls' criterion]] of maximizing the utility of the person who is worse off in society. Further, suppose that the government needs $g$ per head to finance a public good. What is the effect of $g$ on the optimal marginal tax rate?

#+BEGIN_SRC ipython :exports none
def Welfare_g(τ,g):
    τ_0 = np.mean([τ*effort(c,τ) for c in effort_costs])-g
    return np.min([((1-τ)*effort(c,τ)+τ_0 - c*effort(c,τ)**2) for c in effort_costs])

plt.plot(range_tax,[Welfare_g(τ,0.01) for τ in range_tax], label="$g=0$")
plt.plot(range_tax,[Welfare_g(τ,0.05) for τ in range_tax], label="$g=1$")
plt.xlabel('$\\tau$')
plt.ylabel('$W$')
plt.legend()
plt.show()



#+END_SRC

#+RESULTS:
:RESULTS:
# Out[103]:
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-Ro9QuC.png]]
:END:


:ANSWER:
There is no effect of $g$ on $\tau$. The planner already maximizes the utility of the person who is worse off (disregarding everyone else's utility). An increase in $g$ does not affect this trade off and hence there is no effect on $\tau$.
:END:


* Financial crisis

Tirole chapters: 11, 12

We will look at two aspects of the financial crisis. First, why are financial markets problematic in the first place. Second, many people claim that the crisis was (partly) caused by the bonus contracts used by banks. Why do banks offer their employees such contracts?


** Why is there a problem in financial markets?

The first problem in financial (and other) markets is limited liability. When banks go bankrupt they "only" loose their equity even if the debts that they accumulated exceed their equity. We run some simulations to show that this leads to banks taking excessive risks from a social point of view.

Let $x$ denote an investment opportunity: $x$ is a vector with dimension 1000. That is, we assume that there are 1000 states of the world and $x$ gives us the return in each of these states of the world. To find the expected ~profit~, we take the average over the return in all these states of the world. However, if $x$ is "very negative" (a big loss), the bank goes bankrupt and the owners only loose their ~equity~.

#+BEGIN_SRC ipython
def profit(x,equity=0):
    return np.mean(np.maximum(x,-equity))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[1]:
:END:

**Question** Plot the function ~profit~ for the case where ~equity~ =10 and $x$ is a scalar.

#+BEGIN_SRC ipython :exports none
x_values = np.arange(-40,20,0.1)
plt.plot(x_values,[profit(x,10) for x in x_values])
plt.xlabel('$x$')
plt.ylabel('profit')
plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[6]:
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-1xFx9T.png]]
:END:


Note that although we think of $x$ as a vector (and we will use this in a second), python does not know this nor does it care. Hence, we can plot ~profit~ as a function of the scalar $x$.


**Question** Create an investment opportunity ~vector_returns~ where the returns are normally distributed with mean $-10$ and standard deviation 100. As mentioned above, we assume that there are 1000 states of the world.

#+BEGIN_SRC ipython :exports none  
vector_returns = pm.Normal.dist(-10,100).random(size=1000)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[7]:
:END:

**Question** Calculate the expected (social) return from this investment ~vector_returns~.

#+BEGIN_SRC ipython :exports none
np.mean(vector_returns)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[8]:
# text/plain
: -13.147758605065489
:END:

**Question** Calculate the expected profits of the ~vector_returns~. Compare the outcome to the one above. What is the interpretation?

#+BEGIN_SRC ipython  :exports none
profit(vector_returns)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[9]:
# text/plain
: 32.991619442843046
:END:


**Question** Explain what is coded in the following code cell. E.g. what is the point of the 10000?

#+BEGIN_SRC ipython
v_std = np.arange(0,200,1)
v_returns = [pm.Normal.dist(-10,std).random(size=1000) for std in v_std]
plt.scatter([np.std(vx) for vx in v_returns],[profit(vx) for vx in v_returns], label="no equity")
plt.scatter([np.std(vx) for vx in v_returns],[profit(vx,60) for vx in v_returns], label="equity equals 60")
plt.scatter([np.std(vx) for vx in v_returns],[profit(vx,10000) for vx in v_returns], label="social value")
plt.xlabel('$\sigma$')
plt.ylabel('return')
plt.legend()
plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[10]:
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-dS3Jsx.png]]
:END:

**Question** Explain the economic intuition/interpretation of the graph above.

** Why these bonus contracts?

Bonus contracts are generally a reaction to [[Asymmetric information][asymmetric information]]. Let's first consider moral hazard.

*** moral hazard

How do your employees choose their investment opportunities? There are a number of things that you may worry about. Suppose you would pay everyone a fixed salary that does not depend on performance. Then your employees may randomly pick an investment opportunity without analyzing whether this is the best opportunity. The rest of the time, they simply enjoy the sun. Or they may invest your money in the new webshop of their brother in law. This may increase their status in their family but does not necessarily boost your profits. 

Since it is hard for banks to monitor exactly what investment opportunities their employees choose and how risky these are, it seems a good idea to give them some incentive to choose the right investment. One way to do this is to reward good outcomes. That is, the higher the return is, the higher their income will be. That is, employees get a bonus for good /outcomes/. Not for investments with a high expected outcome because that is hard to monitor.

cite:tirole_2017 claims that before the financial crisis investments bank offered high bonuses to attract talented employees (page 345). These bonuses led to inefficient risky behaviour by these employees. But why should competition for talent lead to inefficiencies?

We follow cite:bijlsma2018 to model this question. The structure of this model is comparable to our analysis of [[moral hazard: optimal taxation]] above. For a given bonus contract, employees choose the investment project that maximizes their own income. Knowing this, the bank sets the bonus contract to maximize its profits.

Assume that there are 3 states of the world: the good state where the bank receives $y_g$ as return on the investment; a bad state where the bank makes a loss $y_b <0$ on the project and an "average state" where the bank earns $y_a \in \langle 0, y_g \rangle$. The employee can choose from a set of investment projects that differ in their probabilities over these 3 states of the world. We model this as follows. 

Projects are indexed with their probability $q \in [0,1]$ of the "average state". For a given $q$, the probabilities of the other two states are given by $q_g = \alpha (1-q)(1+q)$ where $\alpha \in [0,0.5]$ denotes the "talent" of the employee and $q_b = 1- q - q_g$ resp. More talented agents (higher $\alpha$) have a higher probability of the good state and a lower probability of the bad state for a given probability $q_a = q$.

The bank cannot observe the project choice $q$ of the employee but it can observe and contract upon the outcome $y_{g,a,b}$. Hence, it can specify a wage for each state $w_{g,a,b}$. The limited liability of the agent is modelled as $w_g,w_a,w_b \geq 0$. The bank cannot fine ($w<0$) its employee. 

**Question** What does a contract with a constant wage look like?

We specify values for $y_{g,a,b}$, define the functions for $q_g$ and $q_b$. 

#+BEGIN_SRC ipython
y_a = 1
y_g = 10
y_b = -20

def q_g(q,ability):
    return ability*(1-q)*(1+q)

def q_b(q,ability):
    return 1 - q - q_g(q,ability)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[11]:
:END:

**Question** Define a function ~q_choice~ of the wage vector $w$ and ~ability~ of the employee. [hint: the vector $w$ only needs to have 2 dimensions]

#+BEGIN_SRC ipython :exports none
def q_choice(w,ability): # w = [w_a,w_g]
    choice = optimize.fminbound(lambda x: -(q_g(x,ability)*w[1] + x*w[0]),0,1,disp=0) #note the minus sign in front of the lambda function
    return choice

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[12]:
:END:

Given the function ~q_choice~, the function ~bank_choices~ derives the optimal wage vector $w=[w_a,w_g]$ for a given ~ability~ of the employee. Then contract derives optimal risk choice $q$ as a function of ~ability~ and the outside option of the employee. If the outside option of the employee is so high that it is no longer profitable for the bank to hire this employee, the function returns $-1$.

#+BEGIN_SRC ipython 
initial_guess = [0.5,1.5]

def bank_choices(ability):
    opt_w = optimize.fmin(lambda x: -(q_g(q_choice(x,ability),ability)*y_g + q_choice(x,ability)*y_a + (1-q_choice(x,ability)-q_g(q_choice(x,ability),ability))*y_b),initial_guess, disp=0)
    return [opt_w,q_choice(opt_w,ability)]

def contract(ability,outside_option):
    q = bank_choices(ability)[1]
    profit = q*y_a + q_g(q,ability)*y_g + q_b(q,ability)*y_b
    if profit - outside_option >= 0:
        q_out = q
    else:
        q_out = -1
    return q_out

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[13]:
:END:

*Question* Plot the optimal $q$ for the bank as a function of the outside option of an employee with ~ability~ equal to $0.5$. Do this for outside options between 0 and 4.

#+BEGIN_SRC ipython :exports none
range_outside_options = np.arange(0,4.01,0.01)
plt.plot(range_outside_options, [contract(0.5,o) for o in range_outside_options])
plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[14]:
# text/plain
: [<matplotlib.lines.Line2D at 0x1c1dc57a58>]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-le6soh.png]]
:END:


*Question* Do we see that as competition for talented employees intensifies, thereby increasing their outside options, banks tend to offer them more risky contracts? Why (not)?

:ANSWER:
Hence, we see that more competition for talented traders which increases their outside options (offers from other banks) does not increase the risks that the banks induce traders to take. Hence, we need to add something to the story to make sense of this. Therefore we will elaborate the model.
:END:

Now we add the second form of asymmetric information: adverse selection. Before we do this and in order to speed up the code below, we will also solve the problem above analytically.

The bank solves the following optimization problem:
\begin{equation}
\label{eq:5}
\max_q qy_a + \alpha (1-q)(1+q)y_g + (1-q-\alpha (1-q)(1+q)) y_b
\end{equation}

*Question* By taking the first order condition for $q$, show that the bank would like to implement:
\begin{equation}
\label{eq:6}
q = \frac{y_a-y_b}{2\alpha(y_g-y_b)}
\end{equation}
What is the optimal $q$ for a trader with $\alpha=0.5$?

*Question* For a social planner, the damage in the bad state ($y_b$) may be bigger (i.e. more negative) than for the bank. The bank only loses its equity, the government may need to bail out the bank or there can be a bank run if one bank collapses. What is the effect on the optimal $q$ of lower (more negative) $y_b$?

*Question* A less talented trader has lower $\alpha$; what is the effect on $q$?

*Question* Use equation (ref:eq:6) to find the $q$ chosen by a worker facing a bonus contract with $w_a,w_g$.

*** moral hazard and adverse selection

Suppose now that there are two types of traders: one talented and the other less so. The bank cannot distinguish these types by just observing them. They may have the same education degrees and work experience. But some have a "knack" for observing opportunities and taking risks, which the others lack.

The bank is willing to pay a lot to remunerate the top traders, but not the average ones. However, the average ones will try to look like the top traders to also earn these stellar incomes. To avoid the average ones to take these risks, the bank needs to pay them enough to stop them from mimicking the top traders.

To simplify the analysis here, we make a number of assumptions:
+ first, the fraction of top traders equals 0.5
+ the outside option of the average traders is so low that top traders are never interested in mimicking average traders; but average traders do want to mimic top traders
+ top traders are only paid in terms of bonus payments ($w_a,w_g$); not a fixed income component
+ when we optimize over the top trader's wages below, we take the $q$ for the average traders and the profit they generate as given (and equal to their optimal profit for the bank)

The last two points can actually be proved (in this sense, they are not assumptions); but we are not going to worry about this here.

Since top traders only receive bonus payments, it must be the case that:
\begin{equation}
\label{eq:7}
\text{outside_option}=qw_a + \alpha_h (1-q)(1+q) w_g
\end{equation}
Defining the bonus ratio are $R=w_a/w_g$, we see that
\begin{equation}
\label{eq:8}
w_g = \frac{\text{outside_option}}{qR+\alpha_h(1-q)(1+q)}
\end{equation}
and hence we find that $w_a = R w_g$.

The code below derives the optimal value of this bonus ratio $R$.

*Question* Explain why $R$ can be interpreted as the "riskiness" of the bonus contract.

#+BEGIN_SRC ipython
α_l = 0.1
α_h = 0.5

def profit(R,outside_option):
    q = R/(2*α_h)
    w_g = outside_option/(R*q + q_g(q,α_h))
    mimic_q = R/(2*α_l)
    w_a = R*w_g
    wage_l = mimic_q*w_a+q_g(mimic_q,α_l)*w_g
    profit = 0.5*(q*y_a + q_g(q,α_h)*y_g+(1-q-q_g(q,α_h))*y_b - outside_option) - 0.5*wage_l
    return [profit, q]

initial_guess = 0.5

def outcome_h(outside_option):
    wages_h = optimize.fmin(lambda x: -profit(x,outside_option)[0],initial_guess, disp=0)
    return profit(wages_h,outside_option)[1]
   
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[15]:
:END:

Finally, we plot the optimal value of $q$ as a function of the outside option.
#+BEGIN_SRC ipython
plt.plot(range_outside_options,[outcome_h(o) for o in range_outside_options])
plt.xlabel('outside option')
plt.ylabel('probability of safe choice $q_a$')
plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[18]:
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-JWaUsS.png]]
:END:



*Question* What does the graph above show? What is the intuition for this?



:ANSWER:
The graph shows that as competition for top traders increases their outside options, banks induce them to take more risks. Even more risk than is optimal for the bank itself.

The intuition for this is: start at first best bonus $w$ for the top traders. Distorting $w$ towards more risk only has a second order effect on profits from top traders. However, since average traders are not so good at taking risks, it allows the bank to reduce the wage paid to these average traders. This is a first order effect.

In other words, part of the higher remuneration that is paid to top traders "leaks away" to average traders. To prevent this leak, more is paid out in the good state ($w_g$ is increased) as top traders are better in getting this good outcome.
:END:

* Using python for empirical research

We consider two ways in which python can be useful for empirical research. First, the use of API's to download datasets. Second, the use of hacker statistics.

** API's to get data

A good reason to use python for data analysis is the option to get on-line data directly into your notebook without going to the website first to download this data. A number of institutes have such python API's. To illustrate this, we use the Worldbank API as described on [[http://wbdata.readthedocs.io/en/latest/][this website]].

The advantage of doing your analysis in this way is that your research becomes better reproducible. Everyone can run the same code and then go through your code of data cleaning steps to end up with the same data set. If instead you first download the data to your computer, then use excel to clean the data and then start analyzing it (say, with stata), no one will be able to reproduce the exact steps that you have taken.

To illustrate the Worldbank API, we will look at the development over time of inequality in gdp per head. So we want measures of gdp per head. The API allows us to search for such indicators in the Worldbank data set. The column on the left gives the name of the variables (that we will use below to download the data); the column on the right explains what the variable provides.

#+BEGIN_SRC ipython
wb.search_indicators("gdp per capita")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[19]:
# output
: 6.0.GDPpc_constant      	GDP per capita, PPP (constant 2011 international $) 
: FB.DPT.INSU.PC.ZS       	Deposit insurance coverage (% of GDP per capita)
: NV.AGR.PCAP.KD.ZG       	Real agricultural GDP per capita growth rate (%)
: NY.GDP.PCAP.PP.KD.ZG    	GDP per capita, PPP annual growth (%)
: NY.GDP.PCAP.PP.KD.87    	GDP per capita, PPP (constant 1987 international $)
: NY.GDP.PCAP.PP.KD       	GDP per capita, PPP (constant 2011 international $)
: NY.GDP.PCAP.PP.CD       	GDP per capita, PPP (current international $)
: NY.GDP.PCAP.KN          	GDP per capita (constant LCU)
: NY.GDP.PCAP.KD.ZG       	GDP per capita growth (annual %)
: NY.GDP.PCAP.KD          	GDP per capita (constant 2010 US$)
: NY.GDP.PCAP.CN          	GDP per capita (current LCU)
: NY.GDP.PCAP.CD          	GDP per capita (current US$)
: SE.XPD.TERT.PC.ZS       	Government expenditure per student, tertiary (% of GDP per capita)
: SE.XPD.SECO.PC.ZS       	Government expenditure per student, secondary (% of GDP per capita)
: SE.XPD.PRIM.PC.ZS       	Government expenditure per student, primary (% of GDP per capita)
: UIS.XUNIT.GDPCAP.4.FSGOV	Government expenditure per post-secondary non-tertiary student as % of GDP per capita (%)
: UIS.XUNIT.GDPCAP.3.FSGOV	Government expenditure per upper secondary student as % of GDP per capita (%)
: UIS.XUNIT.GDPCAP.2.FSGOV	Government expenditure per lower secondary student as % of GDP per capita (%)
: 
:END:

Let's say that we are interested in "GDP per capita, PPP (constant 2011 international $)", we specify this indicator in a dictionary where the key is the "official name" of the variable and the value is the way that we want to refer to the variable (in this case: "GDP_per_head").

With ~get_dataframe~ we actually download the data into the dataframe ~df_wb~. We reset the index in this case (just see what happens to the dataframe if you don't do this). And we look at the first 5 rows to get an idea of what the data are.

#+BEGIN_SRC ipython :exports text/org
indicators = {"NY.GDP.PCAP.PP.KD": "GDP_per_head"}
df_wb = wb.get_dataframe(indicators, convert_date=True)
df_wb.reset_index(inplace = True)
df_wb.head()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[20]:
# text/plain
:       country       date  GDP_per_head
: 0  Arab World 2017-01-01  15413.791998
: 1  Arab World 2016-01-01  15500.530523
: 2  Arab World 2015-01-01  15342.766482
: 3  Arab World 2014-01-01  15199.008915
: 4  Arab World 2013-01-01  15174.101703

# text/html
#+BEGIN_EXPORT html
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>country</th>
      <th>date</th>
      <th>GDP_per_head</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Arab World</td>
      <td>2017-01-01</td>
      <td>15413.791998</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Arab World</td>
      <td>2016-01-01</td>
      <td>15500.530523</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Arab World</td>
      <td>2015-01-01</td>
      <td>15342.766482</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Arab World</td>
      <td>2014-01-01</td>
      <td>15199.008915</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Arab World</td>
      <td>2013-01-01</td>
      <td>15174.101703</td>
    </tr>
  </tbody>
</table>
</div>
#+END_EXPORT
:END:


*Question* What do the last 10 rows look like?

#+BEGIN_SRC ipython :exports none
df_wb.tail(10)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[21]:
# text/plain
:         country       date  GDP_per_head
: 15302  Zimbabwe 1969-01-01           NaN
: 15303  Zimbabwe 1968-01-01           NaN
: 15304  Zimbabwe 1967-01-01           NaN
: 15305  Zimbabwe 1966-01-01           NaN
: 15306  Zimbabwe 1965-01-01           NaN
: 15307  Zimbabwe 1964-01-01           NaN
: 15308  Zimbabwe 1963-01-01           NaN
: 15309  Zimbabwe 1962-01-01           NaN
: 15310  Zimbabwe 1961-01-01           NaN
: 15311  Zimbabwe 1960-01-01           NaN

# text/html
#+BEGIN_EXPORT html
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>country</th>
      <th>date</th>
      <th>GDP_per_head</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>15302</th>
      <td>Zimbabwe</td>
      <td>1969-01-01</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>15303</th>
      <td>Zimbabwe</td>
      <td>1968-01-01</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>15304</th>
      <td>Zimbabwe</td>
      <td>1967-01-01</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>15305</th>
      <td>Zimbabwe</td>
      <td>1966-01-01</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>15306</th>
      <td>Zimbabwe</td>
      <td>1965-01-01</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>15307</th>
      <td>Zimbabwe</td>
      <td>1964-01-01</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>15308</th>
      <td>Zimbabwe</td>
      <td>1963-01-01</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>15309</th>
      <td>Zimbabwe</td>
      <td>1962-01-01</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>15310</th>
      <td>Zimbabwe</td>
      <td>1961-01-01</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>15311</th>
      <td>Zimbabwe</td>
      <td>1960-01-01</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>
#+END_EXPORT
:END:

If you like the dataframe that you have downloaded from the web, you can save it with pandas ~to_csv~. We save the data in the subdirectory "data".

#+BEGIN_SRC ipython
df_wb.to_csv('data/worldbank_data_gdp_per_capita.csv')
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[78]:
:END:


Let's compare the distribution of gdp per head in 1990 with the distribution in 2017. In order to illustrate how we can combine dataframes in pandas, we first define separate dataframes for the years 1990 and 2017.

#+BEGIN_SRC ipython
df_1990=df_wb[df_wb['date']=='1990-01-01']
df_2017=df_wb[df_wb['date']=='2017-01-01']
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[22]:
:END:

*Question* What does the dataframe ~df_1990~ look like?

#+BEGIN_SRC ipython :exports none
df_1990.head()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[23]:
# text/plain
:                             country       date  GDP_per_head
: 27                       Arab World 1990-01-01  10450.208542
: 85           Caribbean small states 1990-01-01   9387.693760
: 143  Central Europe and the Baltics 1990-01-01  12257.927436
: 201      Early-demographic dividend 1990-01-01   4350.079341
: 259             East Asia & Pacific 1990-01-01   4964.741818

# text/html
#+BEGIN_EXPORT html
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>country</th>
      <th>date</th>
      <th>GDP_per_head</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>27</th>
      <td>Arab World</td>
      <td>1990-01-01</td>
      <td>10450.208542</td>
    </tr>
    <tr>
      <th>85</th>
      <td>Caribbean small states</td>
      <td>1990-01-01</td>
      <td>9387.693760</td>
    </tr>
    <tr>
      <th>143</th>
      <td>Central Europe and the Baltics</td>
      <td>1990-01-01</td>
      <td>12257.927436</td>
    </tr>
    <tr>
      <th>201</th>
      <td>Early-demographic dividend</td>
      <td>1990-01-01</td>
      <td>4350.079341</td>
    </tr>
    <tr>
      <th>259</th>
      <td>East Asia &amp; Pacific</td>
      <td>1990-01-01</td>
      <td>4964.741818</td>
    </tr>
  </tbody>
</table>
</div>
#+END_EXPORT
:END:

Both dataframes have a column ~country~. Hence, we can merge the dataframes on this column. There are a number of ~how~ methods, here we use 'inner' which means that only countries that are present in both datasets will be in ~df_merged~. To distinguish the columns, like ~GDP_per_head~ from the two dataframes, we can provide suffixes. All columns from ~df_1990~ (except for ~country~) will be suffixed with '_1990'; and similarly for 2017.

#+BEGIN_SRC ipython
df_merged = pd.merge(df_1990, df_2017, on=['country'], suffixes=['_1990', '_2017'], how='inner')

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[24]:
:END:

*Question* To see how the suffixes work, check what ~df_merged~ looks like.

#+BEGIN_SRC ipython :exports none
df_merged.head()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[25]:
# text/plain
:                           country  date_1990  GDP_per_head_1990  date_2017  \
: 0                      Arab World 1990-01-01       10450.208542 2017-01-01   
: 1          Caribbean small states 1990-01-01        9387.693760 2017-01-01   
: 2  Central Europe and the Baltics 1990-01-01       12257.927436 2017-01-01   
: 3      Early-demographic dividend 1990-01-01        4350.079341 2017-01-01   
: 4             East Asia & Pacific 1990-01-01        4964.741818 2017-01-01   
: 
:    GDP_per_head_2017  
: 0       15413.791998  
: 1       14356.372119  
: 2       26499.126110  
: 3        8857.519723  
: 4       16525.394471  

# text/html
#+BEGIN_EXPORT html
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>country</th>
      <th>date_1990</th>
      <th>GDP_per_head_1990</th>
      <th>date_2017</th>
      <th>GDP_per_head_2017</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Arab World</td>
      <td>1990-01-01</td>
      <td>10450.208542</td>
      <td>2017-01-01</td>
      <td>15413.791998</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Caribbean small states</td>
      <td>1990-01-01</td>
      <td>9387.693760</td>
      <td>2017-01-01</td>
      <td>14356.372119</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Central Europe and the Baltics</td>
      <td>1990-01-01</td>
      <td>12257.927436</td>
      <td>2017-01-01</td>
      <td>26499.126110</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Early-demographic dividend</td>
      <td>1990-01-01</td>
      <td>4350.079341</td>
      <td>2017-01-01</td>
      <td>8857.519723</td>
    </tr>
    <tr>
      <th>4</th>
      <td>East Asia &amp; Pacific</td>
      <td>1990-01-01</td>
      <td>4964.741818</td>
      <td>2017-01-01</td>
      <td>16525.394471</td>
    </tr>
  </tbody>
</table>
</div>
#+END_EXPORT
:END:

*Question* Plot GPD per head in 1990 against GDP per head in 2017. What do you conclude about the development in inequality in these 27 years?


#+BEGIN_SRC ipython :exports none
plt.scatter(df_merged['GDP_per_head_1990'],df_merged['GDP_per_head_2017'])
plt.plot(np.arange(0,100000),np.arange(0,100000))
plt.xlabel('gdp per head in 1990')
plt.ylabel('gdp per head in 2017')
plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[26]:
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-hWZXkH.png]]
:END:


:ANSWER:
If all points would be on the 45-degree line, the distribution of income across countries in 2017 would be the same as in 1990. Instead we see that countries with high incomes in 1990, have even higher incomes in 2017, while this is less the case for countries with low incomes in 1990.
:END:

You may wonder which observations ("dots") correspond to which countries. For this we need a plotting library that is more sophisticated on interactions than ~matplotlib~. A number of these libraries are available; here we consider [[https://bokeh.pydata.org/en/latest/docs/user_guide/quickstart.html][bokeh]]. If you want to know more about bokeh, there is a [[https://www.datacamp.com/courses/interactive-data-visualization-with-bokeh][datacamp course]].

#+BEGIN_SRC ipython
from bokeh.io import output_file, show, output_notebook
from bokeh.plotting import figure
from bokeh.models import HoverTool
output_notebook()

hover = HoverTool(tooltips=[
     ('country','@country'),
     ])

plot = figure(tools=[hover])
plot.circle('GDP_per_head_1990','GDP_per_head_2017',
    size=10, source=df_merged)
output_file('inequality.html')
show(plot)
#+END_SRC

[[./inequality.html]]


** Hacker statistics

In the last chapter of the Datacamp course [[https://www.datacamp.com/courses/intermediate-python-for-data-science][Intermediate Python for Data Science]] you have seen hacker statistics. The idea is to simulate a random process, say, 10,000 times. Then if you want to know the probability that a certain condition is satisfied (say, number of heads bigger than 10 when you throw a coin 15 times), you calculate the number of outcomes where this condition is satisfied and divide it by 10,000.

If you can program, you can recap all the statistics that you were taught (and probably forgot).

*** high school puzzles

Remember when you did statistics at high school? "In a dark cupboard there are 10 red and 20 white socks...", "you throw dice 20 times and ..." etc.

Then you had to do the most counter-intuitive mental jumping around to "understand" why the outcome was "one third".

When you can program, there is no need for this anymore. Program the probability process, replicate it 10,000 times and see what the frequency of an event is.

We will practice this, using the example given in this [[https://www.ted.com/talks/peter_donnelly_shows_how_stats_fool_juries#t-479966][Ted talk]].

+ Experiment 1: you throw a coin a number of times and wait for the pattern "HTH" to emerge;
+ Experiment 2: you throw a coin a number of times and wait for the pattern "HTT" to emerge.

In expectation, which experiment is over sooner (i.e. takes less throws to see the required pattern)?

:ANSWER:
  HTT is "faster": you throw HT (this is good for both patterns). Suppose the next throw is "not good". In exp. 1 you throw T (instead of required H), you have to start all over again in the next throw. In exp. 2 you throw H (instead of required T), you are already 1/3rd into your next sequence of HTT.
:END:

*Question* Define a function ~check~ which checks whether a list ~coin_throws~ ends with the required ~pattern~ (also a list).

In terms of python, you need to use your knowledge of "slicing" lists.


#+BEGIN_SRC ipython :exports none
def check(coin_throws, pattern):
    n = len(pattern)
    return (coin_throws[-n:] == pattern)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[28]:
:END:


*Question* Explain what the following code block does:

#+BEGIN_SRC ipython
outcomes = ['H','T']

def waiting_time(pattern):
    throws = list(np.random.choice(outcomes,size=3))
    while not check(throws,pattern):
        throws.append(random.choice(outcomes))
    return len(throws)

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[29]:
:END:


*Question* Create an empty list ~waiting_times~. Then run the first experiment 50,000 times. Record the waiting time for the pattern ~['H','T','H']~ to appear --for each of these 50,000 experiments-- in the list ~wating_times~. Calculate the average waiting time and give a histogram for the waiting times.

#+BEGIN_SRC ipython :exports none
waiting_times=[]

[waiting_times.append(waiting_time(['H','T','H'])) for i in range(50000)]
print(np.mean(waiting_times))

plt.hist(waiting_times,bins=30)
plt.show()

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[30]:
# output
: 9.94672
: 
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-hEGe96.png]]
:END:


*Question* Do the same as the previous question for experiment 2.

#+BEGIN_SRC ipython :exports none
waiting_times=[]

[waiting_times.append(waiting_time(['H','T','T'])) for i in range(50000)]
print(np.mean(waiting_times))

plt.hist(waiting_times,bins=30)
plt.show()

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[31]:
# output
: 8.03
: 
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-vmZ1OE.png]]
:END:


*Question* Which experiment leads to the lower waiting time in expectation? Do you have an intuition?

*** statistics

Let's start simple. Suppose we have two normally distributed variables $x,y$ with $\mu_x = 10, \mu_y = 20, \sigma_x = 3, \sigma_y =4$. We are interested in the variable $z = x+y$. Suppose you forgot what you know about the sum of normal distributions and wondered what the expectation and standard deviation is of $z$.

*Question* Simulate $z$ and calculate its mean and standard deviation.

#+BEGIN_SRC ipython :exports none
x = pm.Normal.dist(10,3).random(size=10000)
y = pm.Normal.dist(20,4).random(size=10000)
z = x+y
print("mean z: {:.2f}".format(np.mean(z)))
print("std. z: {:.2f}".format(np.std(z)))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[33]:
# output
: mean z: 29.94
: std. z: 4.98
: 
:END:

And, indeed, we know that $\mu_z = \mu_x + \mu_y$ and $\sigma_z = \sqrt{\sigma_x^2 + \sigma_y^2}$

*Question* Let $x$ be normally distributed with $\mu_x=0,\sigma_x=1$ and $y$ has a poisson distribution with $\lambda_y=5$. What is the expectation and standard deviation of $z=xy$?

#+BEGIN_SRC ipython :exports none
x = pm.Normal.dist(0,1).random(size=10000)
y = pm.Poisson.dist(5).random(size=10000)
z = x*y
print("mean z: {:.2f}".format(np.mean(z)))
print("std. z: {:.2f}".format(np.std(z)))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[34]:
# output
: mean z: -0.02
: std. z: 5.48
: 
:END:

Many students find it to hard to think of the distribution of an average or the distribution of a standard deviation.

*Question* Consider the following code block and try to understand what it does.

#+BEGIN_SRC ipython
mu = 1000
sd = 100
number_of_samples=250

def moments(n):
    samples = pm.Normal.dist(mu,sd).random(size=(number_of_samples,n))
    mus = samples.mean(axis=1)
    std = mus.std()
    return [mus,std]
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[35]:
:END:

*Question* Redefine the function ~my_function(n)~ such that it goes through the points in the figure below. [hint: you do not need to fit a function, just use your knowledge of statistics]

#+BEGIN_SRC ipython :exports none
def my_function(n):
    return sd/np.sqrt(n)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[36]:
:END:

#+BEGIN_SRC ipython
def my_function(n):
    return 20
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[37]:
:END:

We plot the second element of the function ~moments~ against $n$ and the function ~my_function~.

#+BEGIN_SRC ipython
range_n = np.arange(1,1000)

plt.plot(range_n,[moments(n)[1] for n in range_n], label='moments')
plt.plot(range_n,[my_function(n) for n in range_n], label='my_function')
plt.legend()
plt.xlabel('$n$')
plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[38]:
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-RUlO5N.png]]
:END:

*Question* Explain what the following distribution below is.

:ANSWER:
This is the distribution of the average of 10 (and 100) draws from a normal distribution with average ~mu~ and standard deviation ~sd~.
:END:

#+BEGIN_SRC ipython
plt.hist(moments(10)[0],bins=30,label='$n=10$',density=True)
plt.hist(moments(100)[0],bins=30,alpha=0.6,label='$n=100$',density=True)
plt.legend()
plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[39]:
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-V4ax40.png]]
:END:

*Question* Compare the probability that the green, blue distribution resp. exceeds 1015.

#+BEGIN_SRC ipython :exports none
print("blue prob. higher than 1015: {:.3f}".format(np.sum(moments(10)[0]>1015)/len(moments(10)[0])))
print("green prob. higher than 1015: {:.3f}".format(np.sum(moments(100)[0]>1015)/len(moments(100)[0])))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[40]:
# output
: blue prob. higher than 1015: 0.300
: green prob. higher than 1015: 0.060
: 
:END:


*Question* Suppose you have a sample of 100 observations. The average of these observations equals 1020. Your hypothesis is that these observations were drawn from a normal distribution with mean 1000 and standard deviation 100. Would you reject this hypothesis?

#+BEGIN_SRC ipython :exports none
(np.sum(moments(100)[0]>1020))/number_of_samples

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[41]:
# text/plain
: 0.02
:END:

#+BEGIN_SRC ipython :exports none
plt.hist(moments(100)[0],bins=30,density=True)
plt.vlines(1020,0,0.06)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[42]:


# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-0Mqr8E.png]]
:END:


If you like this approach, see [[https://www.youtube.com/watch?time_continue=1&v=ssVsVhZEQ9M][this video]] for more examples. There is also a free book (in the form of jupyter notebooks) to [[https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers][recap your statistics]].


* Regulation in health care markets

Tirole: chapters 15, 17

In this section, the main question is: does government regulation have an effect on markets and can we measure/quantify this?

For this we consider the effect of an increase in the deductible $d$ in Dutch basic health insurance.

Some institutional background:
+ we focus on the basic health insurance market (i.e. we ignore the supplementary health insurance market)
+ basic health insurance is mandatory in the Netherlands
+ for people below the age of 18, health care is free of charge
+ for people older than 18: pay the first $d$ euros of treatments per year yourself, treatments above $d$ are free of charge

** simple theory

Consider the following simple theoretical framework. People get
offered at max. one treatment per year. They decide whether either to
accept this treatment or to go without treatment.

The figure below plots costs $c$ of treatment vs. values $v$ of treatments. Each
treatment is a point in this figure; a combination of $c$ and $v$.

The red/blue line is the out-of-pocket payment by an agent facing two deductible levels: 365 and 170 euro resp. Treatments above the red line are always accepted. The value exceeds the out-of-pocket payment for both deductibles. Treatments below the blue line are always rejected: even with the low deductible, the value is below the out-of-pocket payment. Treatments in the yellow area are accepted with the low deductible but are rejected with the high deductible. Hence, to quantify the effect of an increase in the deductible, we want to know the probability that treatments fall in the yellow area. The more treatments in the yellow area, the bigger the fall in health care expenditure in response to an increase in $d$.

#+BEGIN_SRC ipython
def deductible(c,d):
   return min(c,d)

range_c = np.arange(0,500,0.1)
range_v170 = [deductible(c,170) for c in range_c]
range_v365 = [deductible(c,365) for c in range_c]

plt.plot(range_c,range_v365,'-', color = 'r', linewidth = 2, label = '$d=365$')
plt.plot(range_c,range_v170,'-', color = 'b', linewidth = 2, label = '$d=170$')
plt.legend()
plt.fill_between(range_c, range_v170, range_v365, facecolor='yellow')
plt.ylim(0,500)
plt.xlabel('Cost')
plt.ylabel('Value')
plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[55]:
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-HaBHDR.png]]
:END:



** some data

To find the effect of an increase in deductible, we compare health care expenditures in the years 2011 (deductible was 170 euro) and 2014 (deductible was 365 euro). We use data from [[http://www.vektis.nl/index.php/vektis-open-data][Vektis]]. Download from this website the 'csv' files for 2011 and 2014. To use the code below, download these csv-files into the sub-directory "data" (i.e. "data" is sub-directory of the directory in which this notebook resides). 

When you open the csv files, you can see that it uses ";" as separator between columns. Hence, we use pandas' ~read_csv~ statement where we specify the separator as ';'. The data contain total cost per postal code area for a number of cost categories. The expenditures are grouped by sex and age.

The function ~get_data_into_shape~ does a number of things:
+ not all health care cost categories in the data "count" as far as the deductible is concerned. Hence, we select the ones that fall under the deductible and sum these as the relevant total expenditure under the deductible.
+ the cost categories are in Dutch, hence we translate the labels into English
+ we drop variables that we do not need for the analysis here
+ we calculate cost per head per postal code area
+ we also introduce the log of health care costs per head
+ we turn the variable ~sex~ into a category with two values ('M' for males, 'V' for females)
+ we drop the age category '91+' and turn the remaining ages into integers
+ finally, the function returns this new dataframe.

The function illustrates the data manipulation you can do with pandas. We use the same function for the 2011 and 2014 data, which means that we ignore some columns in the 2014 data that are not available in the 2011 data.

#+BEGIN_SRC ipython :exports code
df_2014 = pd.read_csv('data/Vektis Open Databestand Zorgverzekeringswet 2014 - postcode3.csv', sep = ';')

cost_categories_under_deductible = ['KOSTEN_MEDISCH_SPECIALISTISCHE_ZORG', 'KOSTEN_MONDZORG', 'KOSTEN_FARMACIE', 'KOSTEN_HULPMIDDELEN', 'KOSTEN_PARAMEDISCHE_ZORG_FYSIOTHERAPIE', 'KOSTEN_PARAMEDISCHE_ZORG_OVERIG', 'KOSTEN_ZIEKENVERVOER_ZITTEND', 'KOSTEN_ZIEKENVERVOER_LIGGEND', 'KOSTEN_GRENSOVERSCHRIJDENDE_ZORG', 'KOSTEN_OVERIG']

def get_data_into_shape(df):
    df['health_expenditure_under_deductible'] = df[cost_categories_under_deductible].sum(axis=1)
    df = df.rename({
        'GESLACHT':'sex',
        'LEEFTIJDSKLASSE':'age',
        'GEMEENTENAAM':'MUNICIPALITY',
        'AANTAL_BSN':'number_citizens',
        'KOSTEN_MEDISCH_SPECIALISTISCHE_ZORG':'hospital_care',
        'KOSTEN_FARMACIE':'pharmaceuticals',
        'KOSTEN_TWEEDELIJNS_GGZ':'mental_care',
        'KOSTEN_HUISARTS_INSCHRIJFTARIEF':'GP_capitation',
        'KOSTEN_HUISARTS_CONSULT':'GP_fee_for_service',
        'KOSTEN_HUISARTS_OVERIG':'GP_other',
        'KOSTEN_MONDZORG':'dental care',
        'KOSTEN_PARAMEDISCHE_ZORG_FYSIOTHERAPIE':'physiotherapy',
        'KOSTEN_KRAAMZORG':'maternity_care',
        'KOSTEN_VERLOSKUNDIGE_ZORG':'obstetrics'
    }, axis='columns')
    df.drop(['AANTAL_VERZEKERDEJAREN',
             'KOSTEN_HULPMIDDELEN',
             'KOSTEN_PARAMEDISCHE_ZORG_OVERIG',
             'KOSTEN_ZIEKENVERVOER_ZITTEND',
             'KOSTEN_ZIEKENVERVOER_LIGGEND',
             'KOSTEN_GRENSOVERSCHRIJDENDE_ZORG',
             'KOSTEN_OVERIG',
             'KOSTEN_EERSTELIJNS_ONDERSTEUNING'],inplace=True,axis=1)
    df.drop(df.index[[0]], inplace=True)
    df['sex'] = df['sex'].astype('category')
    df['age'] = df['age'].astype('category')
    df['costs_per_head']=df['health_expenditure_under_deductible']/df['number_citizens']
    df['log_costs_per_head']=np.log(1+df['health_expenditure_under_deductible']/df['number_citizens'])
    df = df[(df['age'] != '90+')]
    df['age'] = df['age'].astype(int)
    return df

df_2014 = get_data_into_shape(df_2014)
df_2014.head()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[44]:
# output
: /Users/boone/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2714: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.
:   interactivity=interactivity, compiler=compiler, result=result)
: 
# text/plain
:   sex  age  POSTCODE_3  number_citizens  hospital_care  pharmaceuticals  \
: 1   M    0         0.0              366     1372209.26         31191.20   
: 2   M    0       101.0              590     1682944.17         25898.73   
: 3   M    0       102.0              295     1553933.53         29514.18   
: 4   M    0       103.0              288      827427.31         19263.79   
: 5   M    0       105.0              998     2965316.12         61610.42   
: 
:    KOSTEN_SPECIALISTISCHE_GGZ  GP_capitation  GP_fee_for_service  GP_other  \
: 1                      285.98        5548.60             5540.05  11525.93   
: 2                    20774.91        9816.63            10130.12  20532.03   
: 3                     7970.01        5317.49             6576.70  17426.30   
: 4                      941.40        5014.97             5708.41  14168.90   
: 5                     4780.48       16842.06            19676.01  43794.06   
: 
:    dental care  physiotherapy  maternity_care  obstetrics  \
: 1       681.02       12150.91             0.0         0.0   
: 2         0.00       17777.00             0.0         0.0   
: 3        21.29       20459.17             0.0         0.0   
: 4         0.00        9098.71             0.0         0.0   
: 5       166.98       42332.18             0.0         0.0   
: 
:    KOSTEN_GENERALISTISCHE_BASIS_GGZ  KOSTEN_GERIATRISCHE_REVALIDATIEZORG  \
: 1                               0.0                                  0.0   
: 2                               0.0                                  0.0   
: 3                               0.0                                  0.0   
: 4                               0.0                                  0.0   
: 5                               0.0                                  0.0   
: 
:    health_expenditure_under_deductible  costs_per_head  log_costs_per_head  
: 1                           1425823.15     3895.691667            8.267883  
: 2                           1753560.87     2972.137068            7.997373  
: 3                           1617184.58     5481.981627            8.609404  
: 4                            865867.07     3006.482882            8.008859  
: 5                           3118357.71     3124.606924            8.047384  

# text/html
#+BEGIN_EXPORT html
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sex</th>
      <th>age</th>
      <th>POSTCODE_3</th>
      <th>number_citizens</th>
      <th>hospital_care</th>
      <th>pharmaceuticals</th>
      <th>KOSTEN_SPECIALISTISCHE_GGZ</th>
      <th>GP_capitation</th>
      <th>GP_fee_for_service</th>
      <th>GP_other</th>
      <th>dental care</th>
      <th>physiotherapy</th>
      <th>maternity_care</th>
      <th>obstetrics</th>
      <th>KOSTEN_GENERALISTISCHE_BASIS_GGZ</th>
      <th>KOSTEN_GERIATRISCHE_REVALIDATIEZORG</th>
      <th>health_expenditure_under_deductible</th>
      <th>costs_per_head</th>
      <th>log_costs_per_head</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>M</td>
      <td>0</td>
      <td>0.0</td>
      <td>366</td>
      <td>1372209.26</td>
      <td>31191.20</td>
      <td>285.98</td>
      <td>5548.60</td>
      <td>5540.05</td>
      <td>11525.93</td>
      <td>681.02</td>
      <td>12150.91</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1425823.15</td>
      <td>3895.691667</td>
      <td>8.267883</td>
    </tr>
    <tr>
      <th>2</th>
      <td>M</td>
      <td>0</td>
      <td>101.0</td>
      <td>590</td>
      <td>1682944.17</td>
      <td>25898.73</td>
      <td>20774.91</td>
      <td>9816.63</td>
      <td>10130.12</td>
      <td>20532.03</td>
      <td>0.00</td>
      <td>17777.00</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1753560.87</td>
      <td>2972.137068</td>
      <td>7.997373</td>
    </tr>
    <tr>
      <th>3</th>
      <td>M</td>
      <td>0</td>
      <td>102.0</td>
      <td>295</td>
      <td>1553933.53</td>
      <td>29514.18</td>
      <td>7970.01</td>
      <td>5317.49</td>
      <td>6576.70</td>
      <td>17426.30</td>
      <td>21.29</td>
      <td>20459.17</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1617184.58</td>
      <td>5481.981627</td>
      <td>8.609404</td>
    </tr>
    <tr>
      <th>4</th>
      <td>M</td>
      <td>0</td>
      <td>103.0</td>
      <td>288</td>
      <td>827427.31</td>
      <td>19263.79</td>
      <td>941.40</td>
      <td>5014.97</td>
      <td>5708.41</td>
      <td>14168.90</td>
      <td>0.00</td>
      <td>9098.71</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>865867.07</td>
      <td>3006.482882</td>
      <td>8.008859</td>
    </tr>
    <tr>
      <th>5</th>
      <td>M</td>
      <td>0</td>
      <td>105.0</td>
      <td>998</td>
      <td>2965316.12</td>
      <td>61610.42</td>
      <td>4780.48</td>
      <td>16842.06</td>
      <td>19676.01</td>
      <td>43794.06</td>
      <td>166.98</td>
      <td>42332.18</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>3118357.71</td>
      <td>3124.606924</td>
      <td>8.047384</td>
    </tr>
  </tbody>
</table>
</div>
#+END_EXPORT
:END:

We create ~costs_per_sex_age~ which contains the average health care expenditure (averaged over postal code areas) for each combination of sex and age in the data. For this we use ~pandas~ method ~groupby~.

The outcome of this ~groupby~ we plot below.

#+BEGIN_SRC ipython
costs_per_sex_age = df_2014.groupby(['sex','age'])['costs_per_head'].mean()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[45]:
:END:


** matplotlib

We plot the distribution of health care expenditure per head with age for males and females.

#+BEGIN_SRC ipython
fig = plt.figure()
ax = costs_per_sex_age['M'].plot()
ax = costs_per_sex_age['V'].plot()
ax.set_xlabel('age')
ax.set_ylabel('costs per head')
ax.set_title('average costs per age and sex')
ax.legend(['male','female'])

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[46]:


# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-LlZW0T.png]]
:END:

*Question* Can you interpret how these costs evolve with age and sex?

*Question* How can a graph like this help us to determine the effect of $d$ on health care expenditure?

** reversing the probability distributions

Above we used ~pymc3~ to generate vectors of productivities, valuations, incomes etc. using probability distributions. Here we go the "other way around". We have here distributions of health care expenditures per head and we want to identify the distributions where these come from. To illustrate this, consider the distribution of (average) costs for 30 year old males. Since, health care costs have a skewed distribution, we actually plot the distribution of log costs.

*Question* Plot health care cost distributions for different age and sex categories.

#+BEGIN_SRC ipython
df_2014.query('sex=="M" & age=="30"')['log_costs_per_head'].hist(bins=50)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[47]:


# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-Nm3jNh.png]]
:END:

This distribution looks (sort of) normal. Hence, we assume that for each age and sex category ~log_costs_per_head~ are normally distributed. This implies that ~costs_per_head~ have a log-normal distribution.

*Question* Why are there no zeroes in this figure? Lots of people have no health care expenditure in a year? How does our ~log_costs_per_head~ deal with zero costs? Should there be a spike at 0?

:ANSWER:
This is costs per postal code area. To get zero costs: everyone (of certain age and gender) must have zero costs for the average costs to equal 0. This is quite an unlikely event.
:END:

We focus here on health care costs for women. Clearly, a similar analysis can be done for men. In fact, it is also possible to combine men and women into one analysis with gender fixed effects.

Here we focus on women and introduce age-fixed effects. We assume that observed costs $z$ are ~log_costs_per_head~ which are normally distributed with a mean $\mu$ and standard deviation $\sigma$ which both vary with age. We do not know these means and standard deviations ~μ[age], σ[age]~ but assume they are drawn from prior distributions. A [[https://en.wikipedia.org/wiki/Normal_distribution][normal distribution]] for $\mu$ and a half-normal distribution for $\sigma$.

This is called a Bayesian analysis which you probably never saw before. Do not worry about this; you do not need to understand the details of this analysis. If you find it interesting, there are some references at the end.

#+BEGIN_SRC ipython :async 
log_costs_per_age_female = df_2014[df_2014['sex']=='V'].groupby(['age'])['log_costs_per_head'].mean()

log_costs_per_head = df_2014[df_2014['sex']=='V'].log_costs_per_head.values
age = df_2014[df_2014['sex']=='V'].age.values


with pm.Model() as model:
    
    μ = pm.Normal('μ', 8, 3, shape=len(set(age)))
    σ = pm.HalfNormal('σ', 4, shape=len(set(age)))
    z = pm.Normal('z', μ[age], σ[age], observed=log_costs_per_head)

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[48]:
:END:


#+BEGIN_SRC ipython :async :exports code
with model:
    trace = pm.sample(4000,step = pm.Metropolis(),start = pm.find_MAP())
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[50]:
# output
:   0%|          | 0/5000 [00:00<?, ?it/s]logp = -1.4512e+05, ||grad|| = 6,430.9:   0%|          | 0/5000 [00:00<?, ?it/s]logp = -1.4512e+05, ||grad|| = 6,430.9:   0%|          | 2/5000 [00:00<05:25, 15.34it/s]logp = -48,679, ||grad|| = 13,391:   0%|          | 10/5000 [00:00<05:25, 15.34it/s]    logp = -48,679, ||grad|| = 13,391:   0%|          | 16/5000 [00:00<03:58, 20.90it/s]logp = -32,759, ||grad|| = 683.88:   0%|          | 20/5000 [00:00<03:58, 20.90it/s]logp = -32,663, ||grad|| = 1.6716:   1%|          | 30/5000 [00:00<03:57, 20.90it/s]logp = -32,663, ||grad|| = 1.6716: 100%|██████████| 34/34 [00:00<00:00, 102.41it/s] 
: Multiprocess sampling (4 chains in 4 jobs)
: INFO:pymc3:Multiprocess sampling (4 chains in 4 jobs)
: CompoundStep
: INFO:pymc3:CompoundStep
: >Metropolis: [σ_log__]
: INFO:pymc3:>Metropolis: [σ_log__]
: >Metropolis: [μ]
: INFO:pymc3:>Metropolis: [μ]
:   0%|          | 0/4500 [00:00<?, ?it/s]  0%|          | 1/4500 [00:00<08:41,  8.62it/s]  1%|          | 25/4500 [00:00<00:38, 115.07it/s]  1%|          | 47/4500 [00:00<00:30, 148.02it/s]  2%|▏         | 69/4500 [00:00<00:27, 163.93it/s]  2%|▏         | 91/4500 [00:00<00:25, 173.71it/s]  2%|▏         | 110/4500 [00:00<00:25, 175.39it/s]  3%|▎         | 128/4500 [00:00<00:24, 175.45it/s]  3%|▎         | 146/4500 [00:00<00:24, 175.77it/s]  4%|▎         | 164/4500 [00:00<00:24, 175.85it/s]  4%|▍         | 182/4500 [00:01<00:24, 174.97it/s]  4%|▍         | 201/4500 [00:01<00:24, 176.22it/s]  5%|▍         | 220/4500 [00:01<00:24, 176.80it/s]  5%|▌         | 238/4500 [00:01<00:24, 176.33it/s]  6%|▌         | 257/4500 [00:01<00:23, 176.94it/s]  6%|▌         | 277/4500 [00:01<00:23, 178.26it/s]  7%|▋         | 296/4500 [00:01<00:23, 178.36it/s]  7%|▋         | 315/4500 [00:01<00:23, 178.62it/s]  7%|▋         | 336/4500 [00:01<00:23, 179.93it/s]  8%|▊         | 358/4500 [00:01<00:22, 181.83it/s]  8%|▊         | 379/4500 [00:02<00:22, 183.16it/s]  9%|▉         | 401/4500 [00:02<00:22, 184.51it/s]  9%|▉         | 422/4500 [00:02<00:22, 185.16it/s] 10%|▉         | 445/4500 [00:02<00:21, 186.74it/s] 10%|█         | 467/4500 [00:02<00:21, 187.97it/s] 11%|█         | 489/4500 [00:02<00:21, 189.01it/s] 11%|█▏        | 511/4500 [00:02<00:21, 189.81it/s] 12%|█▏        | 533/4500 [00:02<00:20, 190.41it/s] 12%|█▏        | 554/4500 [00:02<00:20, 189.79it/s] 13%|█▎        | 576/4500 [00:03<00:20, 190.69it/s] 13%|█▎        | 599/4500 [00:03<00:20, 191.68it/s] 14%|█▍        | 622/4500 [00:03<00:20, 192.67it/s] 14%|█▍        | 644/4500 [00:03<00:19, 193.32it/s] 15%|█▍        | 666/4500 [00:03<00:19, 194.03it/s] 15%|█▌        | 689/4500 [00:03<00:19, 194.92it/s] 16%|█▌        | 711/4500 [00:03<00:19, 195.60it/s] 16%|█▋        | 733/4500 [00:03<00:19, 195.92it/s] 17%|█▋        | 755/4500 [00:03<00:19, 196.29it/s] 17%|█▋        | 777/4500 [00:03<00:18, 196.59it/s] 18%|█▊        | 799/4500 [00:04<00:18, 196.92it/s] 18%|█▊        | 821/4500 [00:04<00:18, 197.47it/s] 19%|█▊        | 843/4500 [00:04<00:18, 197.96it/s] 19%|█▉        | 866/4500 [00:04<00:18, 198.51it/s] 20%|█▉        | 889/4500 [00:04<00:18, 199.07it/s] 20%|██        | 911/4500 [00:04<00:17, 199.48it/s] 21%|██        | 933/4500 [00:04<00:17, 199.51it/s] 21%|██        | 955/4500 [00:04<00:17, 199.76it/s] 22%|██▏       | 977/4500 [00:04<00:17, 199.83it/s] 22%|██▏       | 999/4500 [00:04<00:17, 200.09it/s] 23%|██▎       | 1022/4500 [00:05<00:17, 200.52it/s] 23%|██▎       | 1044/4500 [00:05<00:17, 200.27it/s] 24%|██▎       | 1065/4500 [00:05<00:17, 200.31it/s] 24%|██▍       | 1088/4500 [00:05<00:16, 200.72it/s] 25%|██▍       | 1110/4500 [00:05<00:16, 200.98it/s] 25%|██▌       | 1132/4500 [00:05<00:16, 201.29it/s] 26%|██▌       | 1154/4500 [00:05<00:16, 201.57it/s] 26%|██▌       | 1176/4500 [00:05<00:16, 201.63it/s] 27%|██▋       | 1198/4500 [00:05<00:16, 201.88it/s] 27%|██▋       | 1220/4500 [00:06<00:16, 202.14it/s] 28%|██▊       | 1242/4500 [00:06<00:16, 202.27it/s] 28%|██▊       | 1265/4500 [00:06<00:15, 202.59it/s] 29%|██▊       | 1287/4500 [00:06<00:15, 202.81it/s] 29%|██▉       | 1310/4500 [00:06<00:15, 203.13it/s] 30%|██▉       | 1333/4500 [00:06<00:15, 203.49it/s] 30%|███       | 1356/4500 [00:06<00:15, 203.50it/s] 31%|███       | 1378/4500 [00:06<00:15, 203.70it/s] 31%|███       | 1400/4500 [00:06<00:15, 203.74it/s] 32%|███▏      | 1422/4500 [00:06<00:15, 203.81it/s] 32%|███▏      | 1444/4500 [00:07<00:14, 203.83it/s] 33%|███▎      | 1466/4500 [00:07<00:14, 204.03it/s] 33%|███▎      | 1489/4500 [00:07<00:14, 204.32it/s] 34%|███▎      | 1512/4500 [00:07<00:14, 204.55it/s] 34%|███▍      | 1535/4500 [00:07<00:14, 204.85it/s] 35%|███▍      | 1558/4500 [00:07<00:14, 205.04it/s] 35%|███▌      | 1580/4500 [00:07<00:14, 205.21it/s] 36%|███▌      | 1602/4500 [00:07<00:14, 205.20it/s] 36%|███▌      | 1624/4500 [00:07<00:14, 205.21it/s] 37%|███▋      | 1646/4500 [00:08<00:13, 205.37it/s] 37%|███▋      | 1669/4500 [00:08<00:13, 205.63it/s] 38%|███▊      | 1692/4500 [00:08<00:13, 205.82it/s] 38%|███▊      | 1714/4500 [00:08<00:13, 205.94it/s] 39%|███▊      | 1737/4500 [00:08<00:13, 206.19it/s] 39%|███▉      | 1760/4500 [00:08<00:13, 206.36it/s] 40%|███▉      | 1783/4500 [00:08<00:13, 206.52it/s] 40%|████      | 1805/4500 [00:08<00:13, 206.55it/s] 41%|████      | 1827/4500 [00:08<00:12, 206.31it/s] 41%|████      | 1849/4500 [00:08<00:12, 206.45it/s] 42%|████▏     | 1871/4500 [00:09<00:12, 206.60it/s] 42%|████▏     | 1893/4500 [00:09<00:12, 206.72it/s] 43%|████▎     | 1916/4500 [00:09<00:12, 206.89it/s] 43%|████▎     | 1938/4500 [00:09<00:12, 206.81it/s] 44%|████▎     | 1960/4500 [00:09<00:12, 206.75it/s] 44%|████▍     | 1981/4500 [00:09<00:12, 206.55it/s] 44%|████▍     | 2002/4500 [00:09<00:12, 206.53it/s] 45%|████▍     | 2024/4500 [00:09<00:11, 206.54it/s] 45%|████▌     | 2046/4500 [00:09<00:11, 206.61it/s] 46%|████▌     | 2068/4500 [00:10<00:11, 206.71it/s] 46%|████▋     | 2089/4500 [00:10<00:11, 206.62it/s] 47%|████▋     | 2112/4500 [00:10<00:11, 206.79it/s] 47%|████▋     | 2134/4500 [00:10<00:11, 206.91it/s] 48%|████▊     | 2156/4500 [00:10<00:11, 206.57it/s] 48%|████▊     | 2178/4500 [00:10<00:11, 206.68it/s] 49%|████▉     | 2201/4500 [00:10<00:11, 206.82it/s] 49%|████▉     | 2223/4500 [00:10<00:11, 206.75it/s] 50%|████▉     | 2245/4500 [00:10<00:10, 206.83it/s] 50%|█████     | 2267/4500 [00:10<00:10, 206.95it/s] 51%|█████     | 2289/4500 [00:11<00:10, 207.06it/s] 51%|█████▏    | 2312/4500 [00:11<00:10, 207.23it/s] 52%|█████▏    | 2334/4500 [00:11<00:10, 207.33it/s] 52%|█████▏    | 2356/4500 [00:11<00:10, 207.43it/s] 53%|█████▎    | 2379/4500 [00:11<00:10, 207.59it/s] 53%|█████▎    | 2402/4500 [00:11<00:10, 207.70it/s] 54%|█████▍    | 2425/4500 [00:11<00:09, 207.77it/s] 54%|█████▍    | 2448/4500 [00:11<00:09, 207.90it/s] 55%|█████▍    | 2471/4500 [00:11<00:09, 207.66it/s] 55%|█████▌    | 2492/4500 [00:12<00:09, 207.61it/s] 56%|█████▌    | 2515/4500 [00:12<00:09, 207.75it/s] 56%|█████▋    | 2538/4500 [00:12<00:09, 207.88it/s] 57%|█████▋    | 2561/4500 [00:12<00:09, 208.01it/s] 57%|█████▋    | 2583/4500 [00:12<00:09, 208.07it/s] 58%|█████▊    | 2606/4500 [00:12<00:09, 208.19it/s] 58%|█████▊    | 2628/4500 [00:12<00:08, 208.26it/s] 59%|█████▉    | 2650/4500 [00:12<00:08, 208.32it/s] 59%|█████▉    | 2672/4500 [00:12<00:08, 208.23it/s] 60%|█████▉    | 2694/4500 [00:12<00:08, 208.27it/s] 60%|██████    | 2717/4500 [00:13<00:08, 208.37it/s] 61%|██████    | 2740/4500 [00:13<00:08, 208.49it/s] 61%|██████▏   | 2763/4500 [00:13<00:08, 208.61it/s] 62%|██████▏   | 2785/4500 [00:13<00:08, 208.67it/s] 62%|██████▏   | 2808/4500 [00:13<00:08, 208.78it/s] 63%|██████▎   | 2831/4500 [00:13<00:07, 208.91it/s] 63%|██████▎   | 2854/4500 [00:13<00:07, 208.99it/s] 64%|██████▍   | 2877/4500 [00:13<00:07, 208.79it/s] 64%|██████▍   | 2899/4500 [00:13<00:07, 208.85it/s] 65%|██████▍   | 2921/4500 [00:13<00:07, 208.87it/s] 65%|██████▌   | 2943/4500 [00:14<00:07, 208.93it/s] 66%|██████▌   | 2965/4500 [00:14<00:07, 208.97it/s] 66%|██████▋   | 2987/4500 [00:14<00:07, 209.01it/s] 67%|██████▋   | 3009/4500 [00:14<00:07, 208.93it/s] 67%|██████▋   | 3030/4500 [00:14<00:07, 208.88it/s] 68%|██████▊   | 3051/4500 [00:14<00:06, 208.81it/s] 68%|██████▊   | 3072/4500 [00:14<00:06, 208.54it/s] 69%|██████▊   | 3092/4500 [00:14<00:06, 208.30it/s] 69%|██████▉   | 3114/4500 [00:14<00:06, 208.32it/s] 70%|██████▉   | 3137/4500 [00:15<00:06, 208.39it/s] 70%|███████   | 3158/4500 [00:15<00:06, 208.40it/s] 71%|███████   | 3181/4500 [00:15<00:06, 208.49it/s] 71%|███████   | 3203/4500 [00:15<00:06, 208.52it/s] 72%|███████▏  | 3225/4500 [00:15<00:06, 208.49it/s] 72%|███████▏  | 3246/4500 [00:15<00:06, 208.40it/s] 73%|███████▎  | 3267/4500 [00:15<00:05, 208.21it/s] 73%|███████▎  | 3287/4500 [00:15<00:05, 207.93it/s] 73%|███████▎  | 3306/4500 [00:15<00:05, 207.74it/s] 74%|███████▍  | 3325/4500 [00:16<00:05, 207.58it/s] 74%|███████▍  | 3345/4500 [00:16<00:05, 207.49it/s] 75%|███████▍  | 3365/4500 [00:16<00:05, 207.44it/s] 75%|███████▌  | 3386/4500 [00:16<00:05, 207.41it/s] 76%|███████▌  | 3406/4500 [00:16<00:05, 207.33it/s] 76%|███████▌  | 3427/4500 [00:16<00:05, 207.32it/s] 77%|███████▋  | 3447/4500 [00:16<00:05, 207.15it/s] 77%|███████▋  | 3467/4500 [00:16<00:04, 206.87it/s] 77%|███████▋  | 3486/4500 [00:16<00:04, 206.77it/s] 78%|███████▊  | 3507/4500 [00:16<00:04, 206.78it/s] 78%|███████▊  | 3529/4500 [00:17<00:04, 206.82it/s] 79%|███████▉  | 3552/4500 [00:17<00:04, 206.91it/s] 79%|███████▉  | 3575/4500 [00:17<00:04, 207.05it/s] 80%|███████▉  | 3598/4500 [00:17<00:04, 207.17it/s] 80%|████████  | 3621/4500 [00:17<00:04, 207.30it/s] 81%|████████  | 3644/4500 [00:17<00:04, 207.31it/s] 81%|████████▏ | 3667/4500 [00:17<00:04, 207.39it/s] 82%|████████▏ | 3689/4500 [00:17<00:03, 207.36it/s] 82%|████████▏ | 3711/4500 [00:17<00:03, 207.24it/s] 83%|████████▎ | 3732/4500 [00:18<00:03, 207.12it/s] 83%|████████▎ | 3755/4500 [00:18<00:03, 207.22it/s] 84%|████████▍ | 3776/4500 [00:18<00:03, 207.23it/s] 84%|████████▍ | 3797/4500 [00:18<00:03, 207.19it/s] 85%|████████▍ | 3818/4500 [00:18<00:03, 207.17it/s] 85%|████████▌ | 3839/4500 [00:18<00:03, 207.16it/s] 86%|████████▌ | 3860/4500 [00:18<00:03, 207.10it/s] 86%|████████▌ | 3881/4500 [00:18<00:02, 207.04it/s] 87%|████████▋ | 3902/4500 [00:18<00:02, 206.90it/s] 87%|████████▋ | 3922/4500 [00:18<00:02, 206.82it/s] 88%|████████▊ | 3945/4500 [00:19<00:02, 206.92it/s] 88%|████████▊ | 3966/4500 [00:19<00:02, 206.89it/s] 89%|████████▊ | 3987/4500 [00:19<00:02, 206.82it/s] 89%|████████▉ | 4009/4500 [00:19<00:02, 206.86it/s] 90%|████████▉ | 4031/4500 [00:19<00:02, 206.89it/s] 90%|█████████ | 4054/4500 [00:19<00:02, 206.98it/s] 91%|█████████ | 4076/4500 [00:19<00:02, 206.77it/s] 91%|█████████ | 4097/4500 [00:19<00:01, 206.57it/s] 92%|█████████▏| 4120/4500 [00:19<00:01, 206.67it/s] 92%|█████████▏| 4142/4500 [00:20<00:01, 206.72it/s] 93%|█████████▎| 4163/4500 [00:20<00:01, 206.71it/s] 93%|█████████▎| 4186/4500 [00:20<00:01, 206.82it/s] 94%|█████████▎| 4208/4500 [00:20<00:01, 206.80it/s] 94%|█████████▍| 4229/4500 [00:20<00:01, 206.79it/s] 94%|█████████▍| 4250/4500 [00:20<00:01, 206.74it/s] 95%|█████████▍| 4271/4500 [00:20<00:01, 206.73it/s] 95%|█████████▌| 4292/4500 [00:20<00:01, 206.60it/s] 96%|█████████▌| 4312/4500 [00:20<00:00, 206.51it/s] 96%|█████████▋| 4332/4500 [00:20<00:00, 206.45it/s] 97%|█████████▋| 4352/4500 [00:21<00:00, 206.36it/s] 97%|█████████▋| 4373/4500 [00:21<00:00, 206.36it/s] 98%|█████████▊| 4396/4500 [00:21<00:00, 206.46it/s] 98%|█████████▊| 4419/4500 [00:21<00:00, 206.55it/s] 99%|█████████▊| 4442/4500 [00:21<00:00, 206.61it/s] 99%|█████████▉| 4465/4500 [00:21<00:00, 206.71it/s]100%|█████████▉| 4489/4500 [00:21<00:00, 206.84it/s]100%|██████████| 4500/4500 [00:21<00:00, 206.87it/s]
: The gelman-rubin statistic is larger than 1.4 for some parameters. The sampler did not converge.
: ERROR:pymc3:The gelman-rubin statistic is larger than 1.4 for some parameters. The sampler did not converge.
: The estimated number of effective samples is smaller than 200 for some parameters.
: ERROR:pymc3:The estimated number of effective samples is smaller than 200 for some parameters.
: 
:END:


#+BEGIN_SRC ipython :exports code
summary = pm.summary(trace, varnames=['μ'])

pm.plot_posterior(trace, varnames=['μ'],ref_val = log_costs_per_age_female.values)[0]
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[51]:


# text/plain
: <Figure size 864x8100 with 90 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-dgjuhc.png]]
:END:

The figures above compare the estimated distribution for $\mu$ for each age category with the observed average expenditure for this age category in the data. The observed average expenditure falls in the middle of this distribution for each age category. This suggests that the estimation makes some sense (although more tests should be done). 

The "fun" of Bayesian analysis is that we have a (posterior) distribution of the population parameters $\mu$. With "classic" econometrics this is not possible, since $\mu$ is not random in that analysis.

To elaborate a little, in classical econometric analysis we consider the probability $p(\text{data}|\theta)$ where $\theta$ denotes the parameters of the underlying distribution. Hence, we do $p(m>10|\mu=9)$. With Bayes, we do $p(\theta|\text{data})$. The idea is that the data is fixed (we have observed it) but we face uncertainty over the values of the parameters $\theta$. The graphs above plot for each age the distribution of $\mu$.

Now we can plot the average $\mu$ for each age and the observed average expenditure per age category in a graph:

#+BEGIN_SRC ipython
plt.plot(summary['mean'].values,label='calculated means')
plt.plot(log_costs_per_age_female,'o',label='observed means')
plt.legend()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[52]:


# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-uxcjVz.png]]
:END:

To see the effect of the deductible, we compare the average $\mu$ for 17 year olds with the average $\mu$ for 19 year olds:

#+BEGIN_SRC ipython
summary['mean']['μ__17'] - summary['mean']['μ__19']
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[53]:
# text/plain
: 0.29334615011390586
:END:

This is positive: 17 year olds spend more on health care than 19 year olds. Now you may think that although the average $\mu$ is higher for 17 year olds than for 19 year olds, but perhaps it is still likely that $\mu_{19}>\mu_{17}$. Hence we plot these $\mu$ distributions:

#+BEGIN_SRC ipython
plt.hist(trace['μ'][:,17],density=True,label='age 17',bins=50)
plt.hist(trace['μ'][:,19],density=True,label='age 19',bins=50)
plt.legend()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[54]:


# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-KTcJDZ.png]]
:END:

*Question* Does this mean that expenditure of every 17 year old exceeds the expenditure of every 19 year old?


:ANSWER:
No these are the distributions of the parameters $\mu_{17,19}$. Not the individual draws out of the expenditure distributions for 17 and 19 year olds.
:END:

*Questions* Focus on 19 year olds. What is the probability that $\mu_{19}>6.45$?

#+BEGIN_SRC ipython :exports none
np.sum(trace['μ'][:,19]>6.45)/len(trace['μ'][:,19])
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[55]:
# text/plain
: 0.004875
:END:

One explanation for the figure is that 17 year olds do not face a deductible, while 19 year olds do. But an other explanation is that health care expenditure simply differs by age (irrespective of a deductible). In order to control for the age effect, we re-do the analysis above for 2011. Also in 2011 we can take the difference in means for 17 and 19 year olds. If there is (only) a biological reason for the different expenditures between 17 and 19 year olds, the difference in 2011 should be the same as the difference in 2014. 

If, however, the difference in expenditures is caused by the deductible, we expect a bigger difference in 2014 than in 2011 as the deductible was higher in 2014 than in 2011. In terms of our model in section [[simple theory]] above: the yellow area is the additional effect due to the higher deductible in 2014.

Hence, we do the same analysis as above for 2011.

#+BEGIN_SRC ipython :exports code
df_2011 = pd.read_csv('data/Vektis Open Databestand Zorgverzekeringswet 2011 - postcode3.csv', sep = ';')

df_2011 = get_data_into_shape(df_2011)
df_2011.head()

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[56]:
# output
: /Users/boone/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2714: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.
:   interactivity=interactivity, compiler=compiler, result=result)
: 
# text/plain
:   sex  age  POSTCODE_3  number_citizens  hospital_care  pharmaceuticals  \
: 1   M    0         0.0              399      673096.28         24352.91   
: 2   M    0       101.0              608     1141314.40         17499.50   
: 3   M    0       102.0              300      570651.81         15431.84   
: 4   M    0       103.0              287     1459149.63         42044.17   
: 5   M    0       105.0             1049     3036501.62         59187.46   
: 
:    mental_care  GP_capitation  GP_fee_for_service  GP_other  dental care  \
: 1      6249.19        4878.50             5508.93   8312.85          0.0   
: 2      6303.31       10469.99            12216.49  22939.00          0.0   
: 3      6563.82        5346.37             6815.20  13641.15          0.0   
: 4      6348.12        5039.63             6317.01  13070.23          0.0   
: 5     41053.58       18076.34            21496.57  46877.41          0.0   
: 
:    physiotherapy  maternity_care  obstetrics  \
: 1       10708.89             0.0         0.0   
: 2       10272.41             0.0         0.0   
: 3        4090.89             0.0         0.0   
: 4        3732.10             0.0         0.0   
: 5       14180.39             0.0         0.0   
: 
:    KOSTEN_EERSTELIJNS_PSYCHOLOGISCHE_ZORG  \
: 1                                     0.0   
: 2                                     0.0   
: 3                                     0.0   
: 4                                     0.0   
: 5                                     0.0   
: 
:    health_expenditure_under_deductible  costs_per_head  log_costs_per_head  
: 1                            774533.05     1941.185589            7.571569  
: 2                           1196589.65     1968.075082            7.585319  
: 3                            605038.59     2016.795300            7.609761  
: 4                           1661669.25     5789.788328            8.664024  
: 5                           3172935.43     3024.723956            8.014906  

# text/html
#+BEGIN_EXPORT html
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sex</th>
      <th>age</th>
      <th>POSTCODE_3</th>
      <th>number_citizens</th>
      <th>hospital_care</th>
      <th>pharmaceuticals</th>
      <th>mental_care</th>
      <th>GP_capitation</th>
      <th>GP_fee_for_service</th>
      <th>GP_other</th>
      <th>dental care</th>
      <th>physiotherapy</th>
      <th>maternity_care</th>
      <th>obstetrics</th>
      <th>KOSTEN_EERSTELIJNS_PSYCHOLOGISCHE_ZORG</th>
      <th>health_expenditure_under_deductible</th>
      <th>costs_per_head</th>
      <th>log_costs_per_head</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>M</td>
      <td>0</td>
      <td>0.0</td>
      <td>399</td>
      <td>673096.28</td>
      <td>24352.91</td>
      <td>6249.19</td>
      <td>4878.50</td>
      <td>5508.93</td>
      <td>8312.85</td>
      <td>0.0</td>
      <td>10708.89</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>774533.05</td>
      <td>1941.185589</td>
      <td>7.571569</td>
    </tr>
    <tr>
      <th>2</th>
      <td>M</td>
      <td>0</td>
      <td>101.0</td>
      <td>608</td>
      <td>1141314.40</td>
      <td>17499.50</td>
      <td>6303.31</td>
      <td>10469.99</td>
      <td>12216.49</td>
      <td>22939.00</td>
      <td>0.0</td>
      <td>10272.41</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1196589.65</td>
      <td>1968.075082</td>
      <td>7.585319</td>
    </tr>
    <tr>
      <th>3</th>
      <td>M</td>
      <td>0</td>
      <td>102.0</td>
      <td>300</td>
      <td>570651.81</td>
      <td>15431.84</td>
      <td>6563.82</td>
      <td>5346.37</td>
      <td>6815.20</td>
      <td>13641.15</td>
      <td>0.0</td>
      <td>4090.89</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>605038.59</td>
      <td>2016.795300</td>
      <td>7.609761</td>
    </tr>
    <tr>
      <th>4</th>
      <td>M</td>
      <td>0</td>
      <td>103.0</td>
      <td>287</td>
      <td>1459149.63</td>
      <td>42044.17</td>
      <td>6348.12</td>
      <td>5039.63</td>
      <td>6317.01</td>
      <td>13070.23</td>
      <td>0.0</td>
      <td>3732.10</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1661669.25</td>
      <td>5789.788328</td>
      <td>8.664024</td>
    </tr>
    <tr>
      <th>5</th>
      <td>M</td>
      <td>0</td>
      <td>105.0</td>
      <td>1049</td>
      <td>3036501.62</td>
      <td>59187.46</td>
      <td>41053.58</td>
      <td>18076.34</td>
      <td>21496.57</td>
      <td>46877.41</td>
      <td>0.0</td>
      <td>14180.39</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>3172935.43</td>
      <td>3024.723956</td>
      <td>8.014906</td>
    </tr>
  </tbody>
</table>
</div>
#+END_EXPORT
:END:

As above we estimate the model.

#+BEGIN_SRC ipython :async

log_costs_per_age_female = df_2011[df_2011['sex']=='V'].groupby(['age'])['log_costs_per_head'].mean()

log_costs_per_head = df_2011[df_2011['sex']=='V'].log_costs_per_head.values
age = df_2011[df_2011['sex']=='V'].age.values


with pm.Model() as model_2011:
    
    μ = pm.Normal('μ', 8, 3, shape=len(set(age)))
    σ = pm.HalfCauchy('σ', 4, shape=len(set(age)))
    z = pm.Normal('z', μ[age], σ[age], observed=log_costs_per_head)

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[57]:
:END:


#+BEGIN_SRC ipython :async :exports code
with model_2011:
    trace_2011 = pm.sample(4000,step = pm.Metropolis(),start = pm.find_MAP())
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[58]:
# output
:   0%|          | 0/5000 [00:00<?, ?it/s]logp = -1.5898e+05, ||grad|| = 6,668.8:   0%|          | 0/5000 [00:00<?, ?it/s]logp = -38,605, ||grad|| = 4,877.8:   0%|          | 10/5000 [00:00<00:21, 230.40it/s]logp = -26,948, ||grad|| = 36.313:   0%|          | 20/5000 [00:00<00:20, 245.90it/s] logp = -26,948, ||grad|| = 36.313:   1%|          | 26/5000 [00:00<00:19, 259.10it/s]logp = -26,948, ||grad|| = 36.313: 100%|██████████| 30/30 [00:00<00:00, 266.46it/s]  
: Multiprocess sampling (4 chains in 4 jobs)
: INFO:pymc3:Multiprocess sampling (4 chains in 4 jobs)
: CompoundStep
: INFO:pymc3:CompoundStep
: >Metropolis: [σ_log__]
: INFO:pymc3:>Metropolis: [σ_log__]
: >Metropolis: [μ]
: INFO:pymc3:>Metropolis: [μ]
:   0%|          | 0/4500 [00:00<?, ?it/s]  0%|          | 6/4500 [00:00<01:15, 59.89it/s]  1%|          | 31/4500 [00:00<00:29, 151.80it/s]  1%|          | 54/4500 [00:00<00:25, 176.14it/s]  2%|▏         | 77/4500 [00:00<00:23, 187.66it/s]  2%|▏         | 98/4500 [00:00<00:22, 191.92it/s]  3%|▎         | 119/4500 [00:00<00:22, 194.03it/s]  3%|▎         | 139/4500 [00:00<00:22, 193.75it/s]  4%|▎         | 159/4500 [00:00<00:22, 193.90it/s]  4%|▍         | 180/4500 [00:00<00:22, 195.22it/s]  4%|▍         | 201/4500 [00:01<00:21, 196.49it/s]  5%|▍         | 222/4500 [00:01<00:21, 197.38it/s]  5%|▌         | 243/4500 [00:01<00:21, 196.93it/s]  6%|▌         | 264/4500 [00:01<00:21, 197.73it/s]  6%|▋         | 286/4500 [00:01<00:21, 198.75it/s]  7%|▋         | 307/4500 [00:01<00:21, 198.19it/s]  7%|▋         | 328/4500 [00:01<00:21, 197.32it/s]  8%|▊         | 348/4500 [00:01<00:21, 195.65it/s]  8%|▊         | 369/4500 [00:01<00:21, 196.21it/s]  9%|▊         | 389/4500 [00:01<00:21, 195.74it/s]  9%|▉         | 410/4500 [00:02<00:20, 196.05it/s] 10%|▉         | 430/4500 [00:02<00:20, 196.13it/s] 10%|█         | 451/4500 [00:02<00:20, 196.40it/s] 10%|█         | 471/4500 [00:02<00:20, 196.16it/s] 11%|█         | 492/4500 [00:02<00:20, 196.41it/s] 11%|█▏        | 512/4500 [00:02<00:20, 195.69it/s] 12%|█▏        | 532/4500 [00:02<00:20, 195.48it/s] 12%|█▏        | 552/4500 [00:02<00:20, 194.56it/s] 13%|█▎        | 572/4500 [00:02<00:20, 194.66it/s] 13%|█▎        | 593/4500 [00:03<00:20, 195.00it/s] 14%|█▎        | 615/4500 [00:03<00:19, 195.64it/s] 14%|█▍        | 636/4500 [00:03<00:19, 195.59it/s] 15%|█▍        | 656/4500 [00:03<00:19, 195.11it/s] 15%|█▌        | 677/4500 [00:03<00:19, 195.54it/s] 16%|█▌        | 699/4500 [00:03<00:19, 195.93it/s] 16%|█▌        | 720/4500 [00:03<00:19, 194.56it/s] 16%|█▋        | 739/4500 [00:03<00:19, 193.94it/s] 17%|█▋        | 758/4500 [00:03<00:19, 193.01it/s] 17%|█▋        | 777/4500 [00:04<00:19, 192.78it/s] 18%|█▊        | 795/4500 [00:04<00:19, 192.33it/s] 18%|█▊        | 814/4500 [00:04<00:19, 192.26it/s] 19%|█▊        | 836/4500 [00:04<00:19, 192.80it/s] 19%|█▉        | 856/4500 [00:04<00:18, 192.85it/s] 19%|█▉        | 876/4500 [00:04<00:18, 192.83it/s] 20%|█▉        | 896/4500 [00:04<00:18, 192.50it/s] 20%|██        | 915/4500 [00:04<00:18, 191.54it/s] 21%|██        | 934/4500 [00:04<00:18, 191.45it/s] 21%|██        | 953/4500 [00:04<00:18, 191.32it/s] 22%|██▏       | 973/4500 [00:05<00:18, 191.48it/s] 22%|██▏       | 994/4500 [00:05<00:18, 191.80it/s] 23%|██▎       | 1014/4500 [00:05<00:18, 191.84it/s] 23%|██▎       | 1034/4500 [00:05<00:18, 191.76it/s] 23%|██▎       | 1054/4500 [00:05<00:17, 191.54it/s] 24%|██▍       | 1073/4500 [00:05<00:17, 191.37it/s] 24%|██▍       | 1092/4500 [00:05<00:17, 190.37it/s] 25%|██▍       | 1110/4500 [00:05<00:17, 189.85it/s] 25%|██▌       | 1129/4500 [00:05<00:17, 189.78it/s] 26%|██▌       | 1149/4500 [00:06<00:17, 189.82it/s] 26%|██▌       | 1169/4500 [00:06<00:17, 189.92it/s] 26%|██▋       | 1190/4500 [00:06<00:17, 190.17it/s] 27%|██▋       | 1211/4500 [00:06<00:17, 190.44it/s] 27%|██▋       | 1232/4500 [00:06<00:17, 190.61it/s] 28%|██▊       | 1252/4500 [00:06<00:17, 190.70it/s] 28%|██▊       | 1272/4500 [00:06<00:16, 190.80it/s] 29%|██▊       | 1292/4500 [00:06<00:16, 190.64it/s] 29%|██▉       | 1312/4500 [00:06<00:16, 190.63it/s] 30%|██▉       | 1332/4500 [00:06<00:16, 190.59it/s] 30%|███       | 1352/4500 [00:07<00:16, 190.45it/s] 30%|███       | 1371/4500 [00:07<00:16, 189.98it/s] 31%|███       | 1390/4500 [00:07<00:16, 189.57it/s] 31%|███▏      | 1408/4500 [00:07<00:16, 189.11it/s] 32%|███▏      | 1426/4500 [00:07<00:16, 188.60it/s] 32%|███▏      | 1443/4500 [00:07<00:16, 188.06it/s] 32%|███▏      | 1460/4500 [00:07<00:16, 187.71it/s] 33%|███▎      | 1477/4500 [00:07<00:16, 187.43it/s] 33%|███▎      | 1495/4500 [00:07<00:16, 187.25it/s] 34%|███▎      | 1512/4500 [00:08<00:16, 186.73it/s] 34%|███▍      | 1529/4500 [00:08<00:15, 186.23it/s] 34%|███▍      | 1546/4500 [00:08<00:15, 185.93it/s] 35%|███▍      | 1564/4500 [00:08<00:15, 185.78it/s] 35%|███▌      | 1583/4500 [00:08<00:15, 185.83it/s] 36%|███▌      | 1601/4500 [00:08<00:15, 185.73it/s] 36%|███▌      | 1620/4500 [00:08<00:15, 185.76it/s] 36%|███▋      | 1640/4500 [00:08<00:15, 185.91it/s] 37%|███▋      | 1661/4500 [00:08<00:15, 186.16it/s] 37%|███▋      | 1681/4500 [00:09<00:15, 186.11it/s] 38%|███▊      | 1702/4500 [00:09<00:15, 186.30it/s] 38%|███▊      | 1722/4500 [00:09<00:14, 186.45it/s] 39%|███▊      | 1742/4500 [00:09<00:14, 186.40it/s] 39%|███▉      | 1762/4500 [00:09<00:14, 186.54it/s] 40%|███▉      | 1782/4500 [00:09<00:14, 186.44it/s] 40%|████      | 1801/4500 [00:09<00:14, 186.26it/s] 40%|████      | 1820/4500 [00:09<00:14, 186.00it/s] 41%|████      | 1839/4500 [00:09<00:14, 186.03it/s] 41%|████▏     | 1859/4500 [00:09<00:14, 186.07it/s] 42%|████▏     | 1878/4500 [00:10<00:14, 185.97it/s] 42%|████▏     | 1899/4500 [00:10<00:13, 186.13it/s] 43%|████▎     | 1921/4500 [00:10<00:13, 186.39it/s] 43%|████▎     | 1943/4500 [00:10<00:13, 186.64it/s] 44%|████▎     | 1964/4500 [00:10<00:13, 186.76it/s] 44%|████▍     | 1984/4500 [00:10<00:13, 186.74it/s] 45%|████▍     | 2004/4500 [00:10<00:13, 186.78it/s] 45%|████▌     | 2025/4500 [00:10<00:13, 186.94it/s] 45%|████▌     | 2045/4500 [00:10<00:13, 186.96it/s] 46%|████▌     | 2066/4500 [00:11<00:13, 187.09it/s] 46%|████▋     | 2086/4500 [00:11<00:12, 187.14it/s] 47%|████▋     | 2107/4500 [00:11<00:12, 187.29it/s] 47%|████▋     | 2127/4500 [00:11<00:12, 187.35it/s] 48%|████▊     | 2147/4500 [00:11<00:12, 187.41it/s] 48%|████▊     | 2167/4500 [00:11<00:12, 187.44it/s] 49%|████▊     | 2187/4500 [00:11<00:12, 187.17it/s] 49%|████▉     | 2207/4500 [00:11<00:12, 187.22it/s] 49%|████▉     | 2227/4500 [00:11<00:12, 187.30it/s] 50%|████▉     | 2248/4500 [00:11<00:12, 187.40it/s] 50%|█████     | 2268/4500 [00:12<00:11, 187.49it/s] 51%|█████     | 2289/4500 [00:12<00:11, 187.61it/s] 51%|█████▏    | 2310/4500 [00:12<00:11, 187.76it/s] 52%|█████▏    | 2330/4500 [00:12<00:11, 187.84it/s] 52%|█████▏    | 2351/4500 [00:12<00:11, 188.02it/s] 53%|█████▎    | 2372/4500 [00:12<00:11, 188.02it/s] 53%|█████▎    | 2392/4500 [00:12<00:11, 188.03it/s] 54%|█████▎    | 2412/4500 [00:12<00:11, 188.00it/s] 54%|█████▍    | 2432/4500 [00:12<00:10, 188.04it/s] 55%|█████▍    | 2453/4500 [00:13<00:10, 188.16it/s] 55%|█████▌    | 2475/4500 [00:13<00:10, 188.35it/s] 55%|█████▌    | 2497/4500 [00:13<00:10, 188.55it/s] 56%|█████▌    | 2518/4500 [00:13<00:10, 188.69it/s] 56%|█████▋    | 2541/4500 [00:13<00:10, 188.92it/s] 57%|█████▋    | 2562/4500 [00:13<00:10, 189.07it/s] 57%|█████▋    | 2583/4500 [00:13<00:10, 189.20it/s] 58%|█████▊    | 2604/4500 [00:13<00:10, 189.30it/s] 58%|█████▊    | 2625/4500 [00:13<00:09, 189.19it/s] 59%|█████▉    | 2646/4500 [00:13<00:09, 189.28it/s] 59%|█████▉    | 2667/4500 [00:14<00:09, 189.38it/s] 60%|█████▉    | 2687/4500 [00:14<00:09, 189.45it/s] 60%|██████    | 2707/4500 [00:14<00:09, 189.52it/s] 61%|██████    | 2728/4500 [00:14<00:09, 189.64it/s] 61%|██████    | 2749/4500 [00:14<00:09, 189.61it/s] 62%|██████▏   | 2769/4500 [00:14<00:09, 189.55it/s] 62%|██████▏   | 2789/4500 [00:14<00:09, 189.43it/s] 62%|██████▏   | 2808/4500 [00:14<00:08, 189.41it/s] 63%|██████▎   | 2827/4500 [00:14<00:08, 189.35it/s] 63%|██████▎   | 2846/4500 [00:15<00:08, 189.27it/s] 64%|██████▎   | 2865/4500 [00:15<00:08, 189.27it/s] 64%|██████▍   | 2885/4500 [00:15<00:08, 189.33it/s] 65%|██████▍   | 2905/4500 [00:15<00:08, 189.38it/s] 65%|██████▌   | 2926/4500 [00:15<00:08, 189.47it/s] 65%|██████▌   | 2947/4500 [00:15<00:08, 189.59it/s] 66%|██████▌   | 2967/4500 [00:15<00:08, 189.52it/s] 66%|██████▋   | 2987/4500 [00:15<00:07, 189.53it/s] 67%|██████▋   | 3007/4500 [00:15<00:07, 189.36it/s] 67%|██████▋   | 3029/4500 [00:15<00:07, 189.52it/s] 68%|██████▊   | 3049/4500 [00:16<00:07, 189.54it/s] 68%|██████▊   | 3070/4500 [00:16<00:07, 189.64it/s] 69%|██████▊   | 3090/4500 [00:16<00:07, 189.61it/s] 69%|██████▉   | 3112/4500 [00:16<00:07, 189.78it/s] 70%|██████▉   | 3133/4500 [00:16<00:07, 189.86it/s] 70%|███████   | 3154/4500 [00:16<00:07, 189.92it/s] 71%|███████   | 3175/4500 [00:16<00:06, 189.90it/s] 71%|███████   | 3195/4500 [00:16<00:06, 189.85it/s] 71%|███████▏  | 3215/4500 [00:16<00:06, 189.90it/s] 72%|███████▏  | 3235/4500 [00:17<00:06, 189.94it/s] 72%|███████▏  | 3256/4500 [00:17<00:06, 190.04it/s] 73%|███████▎  | 3276/4500 [00:17<00:06, 190.08it/s] 73%|███████▎  | 3296/4500 [00:17<00:06, 190.08it/s] 74%|███████▎  | 3318/4500 [00:17<00:06, 190.21it/s] 74%|███████▍  | 3339/4500 [00:17<00:06, 190.23it/s] 75%|███████▍  | 3359/4500 [00:17<00:05, 190.24it/s] 75%|███████▌  | 3379/4500 [00:17<00:05, 190.24it/s] 76%|███████▌  | 3399/4500 [00:17<00:05, 190.24it/s] 76%|███████▌  | 3419/4500 [00:17<00:05, 190.24it/s] 76%|███████▋  | 3439/4500 [00:18<00:05, 190.25it/s] 77%|███████▋  | 3459/4500 [00:18<00:05, 190.30it/s] 77%|███████▋  | 3479/4500 [00:18<00:05, 190.34it/s] 78%|███████▊  | 3499/4500 [00:18<00:05, 190.33it/s] 78%|███████▊  | 3519/4500 [00:18<00:05, 190.20it/s] 79%|███████▊  | 3538/4500 [00:18<00:05, 190.11it/s] 79%|███████▉  | 3557/4500 [00:18<00:04, 189.94it/s] 79%|███████▉  | 3575/4500 [00:18<00:04, 189.78it/s] 80%|███████▉  | 3595/4500 [00:18<00:04, 189.79it/s] 80%|████████  | 3616/4500 [00:19<00:04, 189.88it/s] 81%|████████  | 3637/4500 [00:19<00:04, 189.95it/s] 81%|████████▏ | 3657/4500 [00:19<00:04, 189.96it/s] 82%|████████▏ | 3678/4500 [00:19<00:04, 190.02it/s] 82%|████████▏ | 3699/4500 [00:19<00:04, 190.10it/s] 83%|████████▎ | 3720/4500 [00:19<00:04, 190.18it/s] 83%|████████▎ | 3740/4500 [00:19<00:03, 190.22it/s] 84%|████████▎ | 3761/4500 [00:19<00:03, 190.28it/s] 84%|████████▍ | 3781/4500 [00:19<00:03, 190.31it/s] 84%|████████▍ | 3801/4500 [00:19<00:03, 190.34it/s] 85%|████████▍ | 3822/4500 [00:20<00:03, 190.42it/s] 85%|████████▌ | 3843/4500 [00:20<00:03, 190.46it/s] 86%|████████▌ | 3863/4500 [00:20<00:03, 190.47it/s] 86%|████████▋ | 3883/4500 [00:20<00:03, 190.45it/s] 87%|████████▋ | 3903/4500 [00:20<00:03, 190.47it/s] 87%|████████▋ | 3925/4500 [00:20<00:03, 190.60it/s] 88%|████████▊ | 3946/4500 [00:20<00:02, 190.49it/s] 88%|████████▊ | 3967/4500 [00:20<00:02, 190.55it/s] 89%|████████▊ | 3988/4500 [00:20<00:02, 190.64it/s] 89%|████████▉ | 4009/4500 [00:21<00:02, 190.73it/s] 90%|████████▉ | 4031/4500 [00:21<00:02, 190.85it/s] 90%|█████████ | 4052/4500 [00:21<00:02, 190.90it/s] 91%|█████████ | 4073/4500 [00:21<00:02, 190.84it/s] 91%|█████████ | 4093/4500 [00:21<00:02, 190.81it/s] 91%|█████████▏| 4113/4500 [00:21<00:02, 190.74it/s] 92%|█████████▏| 4132/4500 [00:21<00:01, 190.58it/s] 92%|█████████▏| 4151/4500 [00:21<00:01, 190.54it/s] 93%|█████████▎| 4171/4500 [00:21<00:01, 190.57it/s] 93%|█████████▎| 4191/4500 [00:21<00:01, 190.59it/s] 94%|█████████▎| 4212/4500 [00:22<00:01, 190.65it/s] 94%|█████████▍| 4232/4500 [00:22<00:01, 190.68it/s] 94%|█████████▍| 4252/4500 [00:22<00:01, 190.57it/s] 95%|█████████▍| 4271/4500 [00:22<00:01, 190.54it/s] 95%|█████████▌| 4292/4500 [00:22<00:01, 190.59it/s] 96%|█████████▌| 4312/4500 [00:22<00:00, 190.57it/s] 96%|█████████▌| 4331/4500 [00:22<00:00, 190.55it/s] 97%|█████████▋| 4350/4500 [00:22<00:00, 190.47it/s] 97%|█████████▋| 4371/4500 [00:22<00:00, 190.55it/s] 98%|█████████▊| 4392/4500 [00:23<00:00, 190.61it/s] 98%|█████████▊| 4412/4500 [00:23<00:00, 190.63it/s] 99%|█████████▊| 4434/4500 [00:23<00:00, 190.73it/s] 99%|█████████▉| 4455/4500 [00:23<00:00, 190.79it/s] 99%|█████████▉| 4476/4500 [00:23<00:00, 190.83it/s]100%|█████████▉| 4497/4500 [00:23<00:00, 190.89it/s]100%|██████████| 4500/4500 [00:23<00:00, 190.90it/s]
: The gelman-rubin statistic is larger than 1.4 for some parameters. The sampler did not converge.
: ERROR:pymc3:The gelman-rubin statistic is larger than 1.4 for some parameters. The sampler did not converge.
: The estimated number of effective samples is smaller than 200 for some parameters.
: ERROR:pymc3:The estimated number of effective samples is smaller than 200 for some parameters.
: 
:END:

#+BEGIN_SRC ipython :exports code
summary_2011 = pm.summary(trace_2011, varnames=['μ'])
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[59]:
:END:

Recall that in 2014 the difference in mean log expenditure equals: 0.29

#+BEGIN_SRC ipython
summary['mean']['μ__17']-summary['mean']['μ__19']
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[60]:
# text/plain
: 0.29334615011390586
:END:


In 2011, this difference is smaller: 0.19

#+BEGIN_SRC ipython
summary_2011['mean']['μ__17']-summary_2011['mean']['μ__19']
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[61]:
# text/plain
: 0.19337257026002064
:END:

Also this we can do with distributions.

#+BEGIN_SRC ipython
plt.hist(trace['μ'][:,17]-trace['μ'][:,19],density=True,label='difference in 2014',bins=50)
plt.hist(trace_2011['μ'][:,17]-trace_2011['μ'][:,19],density=True,label='difference in 2011',bins=50)

plt.legend()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[62]:


# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/48de63ba873b65759d43f92c5813c7a6-Rp85A1.png]]
:END:


*Question* Calculate the probability that in fact the parameter difference $\mu_{17}-\mu_{19}$ is bigger in 2011 than in 2014.

#+BEGIN_SRC ipython :exports none
np.sum(trace_2011['μ'][:,17]-trace_2011['μ'][:,19] > trace['μ'][:,17]-trace['μ'][:,19])/len(trace['μ'][:,19])
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[63]:
# text/plain
: 0.0
:END:

Hence we find that government regulation in terms of a deductible has an effect on health care expenditures. People spend less on health care if they face a higher deductible.

One can extend the analysis above to make it more convincing. Think of things like
+ add more years (using the Vektis website)
+ add year dummies to distinguish year effects (like changes in treatments covered by basic insurance) from changes in deductible
+ add the level of the deductible in the estimation.

If you like this way of modelling with pymc3, you can look at [[https://www.youtube.com/watch?v=TMmSESkhRtI&t=9076s][this video]]

A simple introduction to Bayesian estimation with python can be found [[https://www.youtube.com/watch?v=TpgiFIGXcT4&t=6s][here]]. Allen Downey also has [[http://www.allendowney.com/wp/books/][free books]] on statistics with python.




# Put your references into a file called references.bib
# here we have journal titles defined in a separate file, with aliases in references.bib
bibliographystyle:plainnat
bibliography:~/Dropbox/bibliography/references.bib







#  ov-highlight-data: bmls

# Local Variables:
# eval: (ov-highlight-load)
# End:



